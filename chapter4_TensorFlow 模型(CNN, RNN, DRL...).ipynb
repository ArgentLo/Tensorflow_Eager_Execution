{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if eager_execution enabled\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.40784496],\n",
      "       [1.191065  ],\n",
      "       [1.9742855 ]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.78322077], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(units=1, kernel_initializer=tf.zeros_initializer(),\n",
    "                                           bias_initializer=tf.zeros_initializer())\n",
    "        \n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n",
    "\n",
    "# 以下代码结构与前节类似\n",
    "model = Linear()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)  # 调用模型\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "    \n",
    "print(model.variables)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 基础示例：多层感知机（MLP）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self):\n",
    "        mnist = tf.contrib.learn.datasets.load_dataset('mnist')\n",
    "\n",
    "        self.train_data = mnist.train.images\n",
    "        self.train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "        \n",
    "        self.eval_data = mnist.test.images\n",
    "        self.eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        logits = self(inputs)\n",
    "        return tf.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一些模型超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化模型，数据读取类和优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-a007cc2bb0bf>:4: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From c:\\users\\argent lo\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\argent lo\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\argent lo\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\argent lo\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\argent lo\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\argent lo\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "data_loader = DataLoader()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.415011\n",
      "batch 1: loss 2.256136\n",
      "batch 2: loss 2.175475\n",
      "batch 3: loss 2.027339\n",
      "batch 4: loss 2.002946\n",
      "batch 5: loss 1.963647\n",
      "batch 6: loss 2.043775\n",
      "batch 7: loss 1.860064\n",
      "batch 8: loss 1.597946\n",
      "batch 9: loss 1.639224\n",
      "batch 10: loss 1.570829\n",
      "batch 11: loss 1.518581\n",
      "batch 12: loss 1.607188\n",
      "batch 13: loss 1.518760\n",
      "batch 14: loss 1.384494\n",
      "batch 15: loss 1.354287\n",
      "batch 16: loss 1.446677\n",
      "batch 17: loss 1.389800\n",
      "batch 18: loss 1.118007\n",
      "batch 19: loss 1.405811\n",
      "batch 20: loss 1.326189\n",
      "batch 21: loss 1.171311\n",
      "batch 22: loss 1.086923\n",
      "batch 23: loss 0.965061\n",
      "batch 24: loss 1.125708\n",
      "batch 25: loss 1.123636\n",
      "batch 26: loss 1.050514\n",
      "batch 27: loss 1.031524\n",
      "batch 28: loss 0.863372\n",
      "batch 29: loss 1.050729\n",
      "batch 30: loss 0.865485\n",
      "batch 31: loss 0.834615\n",
      "batch 32: loss 0.839801\n",
      "batch 33: loss 0.744078\n",
      "batch 34: loss 0.989723\n",
      "batch 35: loss 0.739597\n",
      "batch 36: loss 0.740740\n",
      "batch 37: loss 0.745085\n",
      "batch 38: loss 0.990000\n",
      "batch 39: loss 0.945174\n",
      "batch 40: loss 0.775672\n",
      "batch 41: loss 0.776234\n",
      "batch 42: loss 0.590132\n",
      "batch 43: loss 0.756825\n",
      "batch 44: loss 0.672591\n",
      "batch 45: loss 0.861998\n",
      "batch 46: loss 0.887507\n",
      "batch 47: loss 0.662736\n",
      "batch 48: loss 0.505900\n",
      "batch 49: loss 0.728489\n",
      "batch 50: loss 0.542116\n",
      "batch 51: loss 0.574491\n",
      "batch 52: loss 0.598775\n",
      "batch 53: loss 0.529841\n",
      "batch 54: loss 0.571359\n",
      "batch 55: loss 0.545804\n",
      "batch 56: loss 0.579216\n",
      "batch 57: loss 0.623253\n",
      "batch 58: loss 0.519375\n",
      "batch 59: loss 0.528890\n",
      "batch 60: loss 0.826042\n",
      "batch 61: loss 0.593358\n",
      "batch 62: loss 0.514868\n",
      "batch 63: loss 0.509593\n",
      "batch 64: loss 0.696810\n",
      "batch 65: loss 0.648608\n",
      "batch 66: loss 0.501149\n",
      "batch 67: loss 0.496753\n",
      "batch 68: loss 0.588089\n",
      "batch 69: loss 0.449989\n",
      "batch 70: loss 0.568828\n",
      "batch 71: loss 0.342442\n",
      "batch 72: loss 0.559593\n",
      "batch 73: loss 0.385451\n",
      "batch 74: loss 0.623388\n",
      "batch 75: loss 0.244981\n",
      "batch 76: loss 0.463652\n",
      "batch 77: loss 0.580124\n",
      "batch 78: loss 0.474314\n",
      "batch 79: loss 0.482976\n",
      "batch 80: loss 0.450135\n",
      "batch 81: loss 0.467326\n",
      "batch 82: loss 0.318867\n",
      "batch 83: loss 0.533324\n",
      "batch 84: loss 0.511639\n",
      "batch 85: loss 0.530123\n",
      "batch 86: loss 0.504245\n",
      "batch 87: loss 0.328028\n",
      "batch 88: loss 0.484679\n",
      "batch 89: loss 0.237623\n",
      "batch 90: loss 0.277080\n",
      "batch 91: loss 0.439999\n",
      "batch 92: loss 0.548362\n",
      "batch 93: loss 0.433544\n",
      "batch 94: loss 0.448790\n",
      "batch 95: loss 0.333219\n",
      "batch 96: loss 0.490706\n",
      "batch 97: loss 0.654782\n",
      "batch 98: loss 0.365917\n",
      "batch 99: loss 0.576457\n",
      "batch 100: loss 0.470013\n",
      "batch 101: loss 0.320131\n",
      "batch 102: loss 0.412967\n",
      "batch 103: loss 0.581890\n",
      "batch 104: loss 0.542737\n",
      "batch 105: loss 0.410047\n",
      "batch 106: loss 0.370869\n",
      "batch 107: loss 0.578626\n",
      "batch 108: loss 0.323090\n",
      "batch 109: loss 0.359495\n",
      "batch 110: loss 0.709444\n",
      "batch 111: loss 0.418031\n",
      "batch 112: loss 0.337140\n",
      "batch 113: loss 0.471880\n",
      "batch 114: loss 0.532705\n",
      "batch 115: loss 0.483435\n",
      "batch 116: loss 0.327504\n",
      "batch 117: loss 0.266384\n",
      "batch 118: loss 0.725440\n",
      "batch 119: loss 0.384765\n",
      "batch 120: loss 0.356478\n",
      "batch 121: loss 0.278501\n",
      "batch 122: loss 0.448547\n",
      "batch 123: loss 0.280491\n",
      "batch 124: loss 0.511208\n",
      "batch 125: loss 0.270176\n",
      "batch 126: loss 0.367979\n",
      "batch 127: loss 0.647519\n",
      "batch 128: loss 0.354685\n",
      "batch 129: loss 0.303140\n",
      "batch 130: loss 0.495122\n",
      "batch 131: loss 0.642348\n",
      "batch 132: loss 0.286857\n",
      "batch 133: loss 0.609007\n",
      "batch 134: loss 0.534260\n",
      "batch 135: loss 0.312286\n",
      "batch 136: loss 0.400767\n",
      "batch 137: loss 0.483749\n",
      "batch 138: loss 0.473688\n",
      "batch 139: loss 0.523016\n",
      "batch 140: loss 0.212730\n",
      "batch 141: loss 0.336701\n",
      "batch 142: loss 0.575965\n",
      "batch 143: loss 0.480525\n",
      "batch 144: loss 0.363679\n",
      "batch 145: loss 0.459666\n",
      "batch 146: loss 0.256997\n",
      "batch 147: loss 0.438920\n",
      "batch 148: loss 0.354981\n",
      "batch 149: loss 0.329223\n",
      "batch 150: loss 0.161541\n",
      "batch 151: loss 0.404261\n",
      "batch 152: loss 0.256587\n",
      "batch 153: loss 0.258846\n",
      "batch 154: loss 0.343719\n",
      "batch 155: loss 0.234879\n",
      "batch 156: loss 0.239955\n",
      "batch 157: loss 0.390669\n",
      "batch 158: loss 0.487348\n",
      "batch 159: loss 0.266809\n",
      "batch 160: loss 0.252129\n",
      "batch 161: loss 0.221062\n",
      "batch 162: loss 0.408028\n",
      "batch 163: loss 0.416636\n",
      "batch 164: loss 0.316648\n",
      "batch 165: loss 0.361484\n",
      "batch 166: loss 0.306131\n",
      "batch 167: loss 0.402575\n",
      "batch 168: loss 0.272142\n",
      "batch 169: loss 0.197315\n",
      "batch 170: loss 0.279822\n",
      "batch 171: loss 0.494888\n",
      "batch 172: loss 0.341101\n",
      "batch 173: loss 0.616700\n",
      "batch 174: loss 0.719416\n",
      "batch 175: loss 0.288149\n",
      "batch 176: loss 0.447352\n",
      "batch 177: loss 0.338031\n",
      "batch 178: loss 0.548448\n",
      "batch 179: loss 0.318895\n",
      "batch 180: loss 0.356566\n",
      "batch 181: loss 0.277950\n",
      "batch 182: loss 0.358853\n",
      "batch 183: loss 0.272231\n",
      "batch 184: loss 0.369139\n",
      "batch 185: loss 0.400110\n",
      "batch 186: loss 0.268225\n",
      "batch 187: loss 0.442926\n",
      "batch 188: loss 0.395211\n",
      "batch 189: loss 0.243257\n",
      "batch 190: loss 0.220587\n",
      "batch 191: loss 0.447960\n",
      "batch 192: loss 0.414688\n",
      "batch 193: loss 0.509517\n",
      "batch 194: loss 0.348528\n",
      "batch 195: loss 0.525295\n",
      "batch 196: loss 0.460109\n",
      "batch 197: loss 0.299683\n",
      "batch 198: loss 0.369450\n",
      "batch 199: loss 0.299466\n",
      "batch 200: loss 0.482422\n",
      "batch 201: loss 0.262790\n",
      "batch 202: loss 0.493483\n",
      "batch 203: loss 0.466614\n",
      "batch 204: loss 0.378988\n",
      "batch 205: loss 0.283092\n",
      "batch 206: loss 0.254980\n",
      "batch 207: loss 0.392319\n",
      "batch 208: loss 0.228480\n",
      "batch 209: loss 0.272453\n",
      "batch 210: loss 0.520046\n",
      "batch 211: loss 0.216951\n",
      "batch 212: loss 0.374217\n",
      "batch 213: loss 0.424500\n",
      "batch 214: loss 0.377604\n",
      "batch 215: loss 0.189820\n",
      "batch 216: loss 0.193885\n",
      "batch 217: loss 0.613564\n",
      "batch 218: loss 0.254518\n",
      "batch 219: loss 0.208718\n",
      "batch 220: loss 0.602482\n",
      "batch 221: loss 0.515423\n",
      "batch 222: loss 0.316448\n",
      "batch 223: loss 0.289381\n",
      "batch 224: loss 0.485672\n",
      "batch 225: loss 0.443137\n",
      "batch 226: loss 0.217114\n",
      "batch 227: loss 0.505776\n",
      "batch 228: loss 0.346883\n",
      "batch 229: loss 0.441513\n",
      "batch 230: loss 0.392356\n",
      "batch 231: loss 0.236560\n",
      "batch 232: loss 0.182440\n",
      "batch 233: loss 0.291146\n",
      "batch 234: loss 0.301413\n",
      "batch 235: loss 0.327620\n",
      "batch 236: loss 0.287592\n",
      "batch 237: loss 0.502290\n",
      "batch 238: loss 0.360980\n",
      "batch 239: loss 0.402285\n",
      "batch 240: loss 0.200765\n",
      "batch 241: loss 0.369435\n",
      "batch 242: loss 0.411121\n",
      "batch 243: loss 0.253989\n",
      "batch 244: loss 0.339047\n",
      "batch 245: loss 0.222411\n",
      "batch 246: loss 0.200013\n",
      "batch 247: loss 0.231050\n",
      "batch 248: loss 0.476202\n",
      "batch 249: loss 0.298093\n",
      "batch 250: loss 0.521979\n",
      "batch 251: loss 0.410034\n",
      "batch 252: loss 0.515572\n",
      "batch 253: loss 0.416061\n",
      "batch 254: loss 0.316766\n",
      "batch 255: loss 0.273110\n",
      "batch 256: loss 0.288814\n",
      "batch 257: loss 0.224162\n",
      "batch 258: loss 0.287506\n",
      "batch 259: loss 0.220096\n",
      "batch 260: loss 0.198317\n",
      "batch 261: loss 0.391520\n",
      "batch 262: loss 0.396260\n",
      "batch 263: loss 0.183377\n",
      "batch 264: loss 0.391986\n",
      "batch 265: loss 0.331762\n",
      "batch 266: loss 0.234606\n",
      "batch 267: loss 0.232801\n",
      "batch 268: loss 0.158674\n",
      "batch 269: loss 0.482990\n",
      "batch 270: loss 0.172011\n",
      "batch 271: loss 0.155057\n",
      "batch 272: loss 0.170609\n",
      "batch 273: loss 0.202693\n",
      "batch 274: loss 0.239864\n",
      "batch 275: loss 0.496810\n",
      "batch 276: loss 0.233421\n",
      "batch 277: loss 0.312589\n",
      "batch 278: loss 0.271258\n",
      "batch 279: loss 0.265681\n",
      "batch 280: loss 0.208292\n",
      "batch 281: loss 0.304900\n",
      "batch 282: loss 0.382872\n",
      "batch 283: loss 0.217387\n",
      "batch 284: loss 0.395549\n",
      "batch 285: loss 0.164966\n",
      "batch 286: loss 0.333842\n",
      "batch 287: loss 0.305954\n",
      "batch 288: loss 0.419818\n",
      "batch 289: loss 0.246134\n",
      "batch 290: loss 0.136200\n",
      "batch 291: loss 0.503960\n",
      "batch 292: loss 0.120861\n",
      "batch 293: loss 0.251348\n",
      "batch 294: loss 0.375736\n",
      "batch 295: loss 0.220141\n",
      "batch 296: loss 0.362198\n",
      "batch 297: loss 0.312807\n",
      "batch 298: loss 0.162650\n",
      "batch 299: loss 0.288457\n",
      "batch 300: loss 0.247685\n",
      "batch 301: loss 0.428984\n",
      "batch 302: loss 0.239819\n",
      "batch 303: loss 0.253417\n",
      "batch 304: loss 0.313649\n",
      "batch 305: loss 0.612198\n",
      "batch 306: loss 0.166157\n",
      "batch 307: loss 0.319507\n",
      "batch 308: loss 0.313190\n",
      "batch 309: loss 0.288454\n",
      "batch 310: loss 0.182774\n",
      "batch 311: loss 0.283267\n",
      "batch 312: loss 0.302764\n",
      "batch 313: loss 0.261130\n",
      "batch 314: loss 0.252848\n",
      "batch 315: loss 0.349931\n",
      "batch 316: loss 0.223620\n",
      "batch 317: loss 0.204792\n",
      "batch 318: loss 0.261358\n",
      "batch 319: loss 0.237705\n",
      "batch 320: loss 0.284982\n",
      "batch 321: loss 0.338509\n",
      "batch 322: loss 0.521884\n",
      "batch 323: loss 0.216524\n",
      "batch 324: loss 0.673170\n",
      "batch 325: loss 0.416568\n",
      "batch 326: loss 0.294438\n",
      "batch 327: loss 0.257527\n",
      "batch 328: loss 0.195809\n",
      "batch 329: loss 0.333724\n",
      "batch 330: loss 0.335820\n",
      "batch 331: loss 0.690156\n",
      "batch 332: loss 0.227733\n",
      "batch 333: loss 0.187416\n",
      "batch 334: loss 0.234569\n",
      "batch 335: loss 0.219229\n",
      "batch 336: loss 0.470594\n",
      "batch 337: loss 0.137767\n",
      "batch 338: loss 0.368790\n",
      "batch 339: loss 0.246578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 340: loss 0.210189\n",
      "batch 341: loss 0.179342\n",
      "batch 342: loss 0.328257\n",
      "batch 343: loss 0.203322\n",
      "batch 344: loss 0.263793\n",
      "batch 345: loss 0.314159\n",
      "batch 346: loss 0.188632\n",
      "batch 347: loss 0.422263\n",
      "batch 348: loss 0.390266\n",
      "batch 349: loss 0.159236\n",
      "batch 350: loss 0.187802\n",
      "batch 351: loss 0.247852\n",
      "batch 352: loss 0.573666\n",
      "batch 353: loss 0.543177\n",
      "batch 354: loss 0.215195\n",
      "batch 355: loss 0.227147\n",
      "batch 356: loss 0.538908\n",
      "batch 357: loss 0.271371\n",
      "batch 358: loss 0.248778\n",
      "batch 359: loss 0.194033\n",
      "batch 360: loss 0.347144\n",
      "batch 361: loss 0.398778\n",
      "batch 362: loss 0.294433\n",
      "batch 363: loss 0.137400\n",
      "batch 364: loss 0.234213\n",
      "batch 365: loss 0.176707\n",
      "batch 366: loss 0.207019\n",
      "batch 367: loss 0.195892\n",
      "batch 368: loss 0.233113\n",
      "batch 369: loss 0.129160\n",
      "batch 370: loss 0.317945\n",
      "batch 371: loss 0.223531\n",
      "batch 372: loss 0.155966\n",
      "batch 373: loss 0.339252\n",
      "batch 374: loss 0.160594\n",
      "batch 375: loss 0.212604\n",
      "batch 376: loss 0.426228\n",
      "batch 377: loss 0.354688\n",
      "batch 378: loss 0.150215\n",
      "batch 379: loss 0.309177\n",
      "batch 380: loss 0.285746\n",
      "batch 381: loss 0.313109\n",
      "batch 382: loss 0.357597\n",
      "batch 383: loss 0.136412\n",
      "batch 384: loss 0.404289\n",
      "batch 385: loss 0.136418\n",
      "batch 386: loss 0.196091\n",
      "batch 387: loss 0.166594\n",
      "batch 388: loss 0.413588\n",
      "batch 389: loss 0.111188\n",
      "batch 390: loss 0.165005\n",
      "batch 391: loss 0.128576\n",
      "batch 392: loss 0.412020\n",
      "batch 393: loss 0.370826\n",
      "batch 394: loss 0.239260\n",
      "batch 395: loss 0.085181\n",
      "batch 396: loss 0.322956\n",
      "batch 397: loss 0.302726\n",
      "batch 398: loss 0.221935\n",
      "batch 399: loss 0.385528\n",
      "batch 400: loss 0.174663\n",
      "batch 401: loss 0.170314\n",
      "batch 402: loss 0.329750\n",
      "batch 403: loss 0.219939\n",
      "batch 404: loss 0.467549\n",
      "batch 405: loss 0.206822\n",
      "batch 406: loss 0.323906\n",
      "batch 407: loss 0.236865\n",
      "batch 408: loss 0.270483\n",
      "batch 409: loss 0.170847\n",
      "batch 410: loss 0.447027\n",
      "batch 411: loss 0.231331\n",
      "batch 412: loss 0.289509\n",
      "batch 413: loss 0.368529\n",
      "batch 414: loss 0.178383\n",
      "batch 415: loss 0.179032\n",
      "batch 416: loss 0.204035\n",
      "batch 417: loss 0.164379\n",
      "batch 418: loss 0.123513\n",
      "batch 419: loss 0.146338\n",
      "batch 420: loss 0.362606\n",
      "batch 421: loss 0.314128\n",
      "batch 422: loss 0.230128\n",
      "batch 423: loss 0.154395\n",
      "batch 424: loss 0.495086\n",
      "batch 425: loss 0.183487\n",
      "batch 426: loss 0.175511\n",
      "batch 427: loss 0.215237\n",
      "batch 428: loss 0.316356\n",
      "batch 429: loss 0.290526\n",
      "batch 430: loss 0.218845\n",
      "batch 431: loss 0.201120\n",
      "batch 432: loss 0.239384\n",
      "batch 433: loss 0.167927\n",
      "batch 434: loss 0.271254\n",
      "batch 435: loss 0.277517\n",
      "batch 436: loss 0.391550\n",
      "batch 437: loss 0.186687\n",
      "batch 438: loss 0.222327\n",
      "batch 439: loss 0.345087\n",
      "batch 440: loss 0.377955\n",
      "batch 441: loss 0.240016\n",
      "batch 442: loss 0.129700\n",
      "batch 443: loss 0.123002\n",
      "batch 444: loss 0.368876\n",
      "batch 445: loss 0.098053\n",
      "batch 446: loss 0.268240\n",
      "batch 447: loss 0.177807\n",
      "batch 448: loss 0.322095\n",
      "batch 449: loss 0.304140\n",
      "batch 450: loss 0.193478\n",
      "batch 451: loss 0.227448\n",
      "batch 452: loss 0.401206\n",
      "batch 453: loss 0.302328\n",
      "batch 454: loss 0.152538\n",
      "batch 455: loss 0.271122\n",
      "batch 456: loss 0.114547\n",
      "batch 457: loss 0.559713\n",
      "batch 458: loss 0.225185\n",
      "batch 459: loss 0.174380\n",
      "batch 460: loss 0.325252\n",
      "batch 461: loss 0.172109\n",
      "batch 462: loss 0.347850\n",
      "batch 463: loss 0.141980\n",
      "batch 464: loss 0.329611\n",
      "batch 465: loss 0.203349\n",
      "batch 466: loss 0.419866\n",
      "batch 467: loss 0.162232\n",
      "batch 468: loss 0.488581\n",
      "batch 469: loss 0.208355\n",
      "batch 470: loss 0.175809\n",
      "batch 471: loss 0.408033\n",
      "batch 472: loss 0.223418\n",
      "batch 473: loss 0.303271\n",
      "batch 474: loss 0.203090\n",
      "batch 475: loss 0.134695\n",
      "batch 476: loss 0.145264\n",
      "batch 477: loss 0.325273\n",
      "batch 478: loss 0.192993\n",
      "batch 479: loss 0.460392\n",
      "batch 480: loss 0.164009\n",
      "batch 481: loss 0.382804\n",
      "batch 482: loss 0.101491\n",
      "batch 483: loss 0.409173\n",
      "batch 484: loss 0.442987\n",
      "batch 485: loss 0.130902\n",
      "batch 486: loss 0.444071\n",
      "batch 487: loss 0.184581\n",
      "batch 488: loss 0.224019\n",
      "batch 489: loss 0.120646\n",
      "batch 490: loss 0.408427\n",
      "batch 491: loss 0.072564\n",
      "batch 492: loss 0.172502\n",
      "batch 493: loss 0.170596\n",
      "batch 494: loss 0.484066\n",
      "batch 495: loss 0.277063\n",
      "batch 496: loss 0.384809\n",
      "batch 497: loss 0.128043\n",
      "batch 498: loss 0.254939\n",
      "batch 499: loss 0.238182\n",
      "batch 500: loss 0.329409\n",
      "batch 501: loss 0.225591\n",
      "batch 502: loss 0.250397\n",
      "batch 503: loss 0.181920\n",
      "batch 504: loss 0.227888\n",
      "batch 505: loss 0.115430\n",
      "batch 506: loss 0.239656\n",
      "batch 507: loss 0.185839\n",
      "batch 508: loss 0.282082\n",
      "batch 509: loss 0.297231\n",
      "batch 510: loss 0.231048\n",
      "batch 511: loss 0.127157\n",
      "batch 512: loss 0.361610\n",
      "batch 513: loss 0.446869\n",
      "batch 514: loss 0.086837\n",
      "batch 515: loss 0.101875\n",
      "batch 516: loss 0.269279\n",
      "batch 517: loss 0.250351\n",
      "batch 518: loss 0.234941\n",
      "batch 519: loss 0.194702\n",
      "batch 520: loss 0.332267\n",
      "batch 521: loss 0.375102\n",
      "batch 522: loss 0.182007\n",
      "batch 523: loss 0.367057\n",
      "batch 524: loss 0.229560\n",
      "batch 525: loss 0.364824\n",
      "batch 526: loss 0.300044\n",
      "batch 527: loss 0.207106\n",
      "batch 528: loss 0.354001\n",
      "batch 529: loss 0.174647\n",
      "batch 530: loss 0.157145\n",
      "batch 531: loss 0.112806\n",
      "batch 532: loss 0.296884\n",
      "batch 533: loss 0.192224\n",
      "batch 534: loss 0.182347\n",
      "batch 535: loss 0.332114\n",
      "batch 536: loss 0.278638\n",
      "batch 537: loss 0.213124\n",
      "batch 538: loss 0.307043\n",
      "batch 539: loss 0.271972\n",
      "batch 540: loss 0.308505\n",
      "batch 541: loss 0.349499\n",
      "batch 542: loss 0.347687\n",
      "batch 543: loss 0.213143\n",
      "batch 544: loss 0.110201\n",
      "batch 545: loss 0.097536\n",
      "batch 546: loss 0.355638\n",
      "batch 547: loss 0.333196\n",
      "batch 548: loss 0.199263\n",
      "batch 549: loss 0.463179\n",
      "batch 550: loss 0.127305\n",
      "batch 551: loss 0.149373\n",
      "batch 552: loss 0.229860\n",
      "batch 553: loss 0.385964\n",
      "batch 554: loss 0.217306\n",
      "batch 555: loss 0.187122\n",
      "batch 556: loss 0.312540\n",
      "batch 557: loss 0.252804\n",
      "batch 558: loss 0.168693\n",
      "batch 559: loss 0.135931\n",
      "batch 560: loss 0.167095\n",
      "batch 561: loss 0.149560\n",
      "batch 562: loss 0.287375\n",
      "batch 563: loss 0.191309\n",
      "batch 564: loss 0.209982\n",
      "batch 565: loss 0.081217\n",
      "batch 566: loss 0.177466\n",
      "batch 567: loss 0.296922\n",
      "batch 568: loss 0.115061\n",
      "batch 569: loss 0.336345\n",
      "batch 570: loss 0.177177\n",
      "batch 571: loss 0.284062\n",
      "batch 572: loss 0.328696\n",
      "batch 573: loss 0.083880\n",
      "batch 574: loss 0.190874\n",
      "batch 575: loss 0.226460\n",
      "batch 576: loss 0.175752\n",
      "batch 577: loss 0.362980\n",
      "batch 578: loss 0.200584\n",
      "batch 579: loss 0.415825\n",
      "batch 580: loss 0.152809\n",
      "batch 581: loss 0.262115\n",
      "batch 582: loss 0.259429\n",
      "batch 583: loss 0.475781\n",
      "batch 584: loss 0.094972\n",
      "batch 585: loss 0.126744\n",
      "batch 586: loss 0.243098\n",
      "batch 587: loss 0.102254\n",
      "batch 588: loss 0.222642\n",
      "batch 589: loss 0.126322\n",
      "batch 590: loss 0.207635\n",
      "batch 591: loss 0.332055\n",
      "batch 592: loss 0.216360\n",
      "batch 593: loss 0.355544\n",
      "batch 594: loss 0.239076\n",
      "batch 595: loss 0.284337\n",
      "batch 596: loss 0.232417\n",
      "batch 597: loss 0.167454\n",
      "batch 598: loss 0.287497\n",
      "batch 599: loss 0.188799\n",
      "batch 600: loss 0.182134\n",
      "batch 601: loss 0.356719\n",
      "batch 602: loss 0.277263\n",
      "batch 603: loss 0.358898\n",
      "batch 604: loss 0.236115\n",
      "batch 605: loss 0.235844\n",
      "batch 606: loss 0.230323\n",
      "batch 607: loss 0.256695\n",
      "batch 608: loss 0.310629\n",
      "batch 609: loss 0.113503\n",
      "batch 610: loss 0.401007\n",
      "batch 611: loss 0.138320\n",
      "batch 612: loss 0.206436\n",
      "batch 613: loss 0.287722\n",
      "batch 614: loss 0.109237\n",
      "batch 615: loss 0.192458\n",
      "batch 616: loss 0.201085\n",
      "batch 617: loss 0.399376\n",
      "batch 618: loss 0.145298\n",
      "batch 619: loss 0.120655\n",
      "batch 620: loss 0.210266\n",
      "batch 621: loss 0.157442\n",
      "batch 622: loss 0.207929\n",
      "batch 623: loss 0.199453\n",
      "batch 624: loss 0.145515\n",
      "batch 625: loss 0.135953\n",
      "batch 626: loss 0.328037\n",
      "batch 627: loss 0.098593\n",
      "batch 628: loss 0.234473\n",
      "batch 629: loss 0.082329\n",
      "batch 630: loss 0.183123\n",
      "batch 631: loss 0.052376\n",
      "batch 632: loss 0.184408\n",
      "batch 633: loss 0.331503\n",
      "batch 634: loss 0.134087\n",
      "batch 635: loss 0.224974\n",
      "batch 636: loss 0.258937\n",
      "batch 637: loss 0.225753\n",
      "batch 638: loss 0.113978\n",
      "batch 639: loss 0.229864\n",
      "batch 640: loss 0.118377\n",
      "batch 641: loss 0.196259\n",
      "batch 642: loss 0.117226\n",
      "batch 643: loss 0.393686\n",
      "batch 644: loss 0.209039\n",
      "batch 645: loss 0.089780\n",
      "batch 646: loss 0.101950\n",
      "batch 647: loss 0.364020\n",
      "batch 648: loss 0.223208\n",
      "batch 649: loss 0.201881\n",
      "batch 650: loss 0.268692\n",
      "batch 651: loss 0.106470\n",
      "batch 652: loss 0.183359\n",
      "batch 653: loss 0.098275\n",
      "batch 654: loss 0.338465\n",
      "batch 655: loss 0.145432\n",
      "batch 656: loss 0.327118\n",
      "batch 657: loss 0.148308\n",
      "batch 658: loss 0.118195\n",
      "batch 659: loss 0.230416\n",
      "batch 660: loss 0.094176\n",
      "batch 661: loss 0.330133\n",
      "batch 662: loss 0.326073\n",
      "batch 663: loss 0.244074\n",
      "batch 664: loss 0.217122\n",
      "batch 665: loss 0.203877\n",
      "batch 666: loss 0.262181\n",
      "batch 667: loss 0.208436\n",
      "batch 668: loss 0.156124\n",
      "batch 669: loss 0.266450\n",
      "batch 670: loss 0.056625\n",
      "batch 671: loss 0.297555\n",
      "batch 672: loss 0.267799\n",
      "batch 673: loss 0.137989\n",
      "batch 674: loss 0.175074\n",
      "batch 675: loss 0.080252\n",
      "batch 676: loss 0.129989\n",
      "batch 677: loss 0.070873\n",
      "batch 678: loss 0.152588\n",
      "batch 679: loss 0.291118\n",
      "batch 680: loss 0.222792\n",
      "batch 681: loss 0.105571\n",
      "batch 682: loss 0.223736\n",
      "batch 683: loss 0.287052\n",
      "batch 684: loss 0.127513\n",
      "batch 685: loss 0.312226\n",
      "batch 686: loss 0.121353\n",
      "batch 687: loss 0.305058\n",
      "batch 688: loss 0.299854\n",
      "batch 689: loss 0.220626\n",
      "batch 690: loss 0.339696\n",
      "batch 691: loss 0.231890\n",
      "batch 692: loss 0.349260\n",
      "batch 693: loss 0.224664\n",
      "batch 694: loss 0.135224\n",
      "batch 695: loss 0.246807\n",
      "batch 696: loss 0.247506\n",
      "batch 697: loss 0.225389\n",
      "batch 698: loss 0.395177\n",
      "batch 699: loss 0.302363\n",
      "batch 700: loss 0.260721\n",
      "batch 701: loss 0.113915\n",
      "batch 702: loss 0.174419\n",
      "batch 703: loss 0.138633\n",
      "batch 704: loss 0.093420\n",
      "batch 705: loss 0.324996\n",
      "batch 706: loss 0.167823\n",
      "batch 707: loss 0.509170\n",
      "batch 708: loss 0.300544\n",
      "batch 709: loss 0.123011\n",
      "batch 710: loss 0.270817\n",
      "batch 711: loss 0.133971\n",
      "batch 712: loss 0.305902\n",
      "batch 713: loss 0.233547\n",
      "batch 714: loss 0.091349\n",
      "batch 715: loss 0.264272\n",
      "batch 716: loss 0.320974\n",
      "batch 717: loss 0.288997\n",
      "batch 718: loss 0.335003\n",
      "batch 719: loss 0.199548\n",
      "batch 720: loss 0.164710\n",
      "batch 721: loss 0.108575\n",
      "batch 722: loss 0.423112\n",
      "batch 723: loss 0.196277\n",
      "batch 724: loss 0.072399\n",
      "batch 725: loss 0.159092\n",
      "batch 726: loss 0.112994\n",
      "batch 727: loss 0.219381\n",
      "batch 728: loss 0.161139\n",
      "batch 729: loss 0.237840\n",
      "batch 730: loss 0.391910\n",
      "batch 731: loss 0.249917\n",
      "batch 732: loss 0.302701\n",
      "batch 733: loss 0.199249\n",
      "batch 734: loss 0.492317\n",
      "batch 735: loss 0.163879\n",
      "batch 736: loss 0.514788\n",
      "batch 737: loss 0.161856\n",
      "batch 738: loss 0.233309\n",
      "batch 739: loss 0.087674\n",
      "batch 740: loss 0.532633\n",
      "batch 741: loss 0.135316\n",
      "batch 742: loss 0.093087\n",
      "batch 743: loss 0.224005\n",
      "batch 744: loss 0.248096\n",
      "batch 745: loss 0.217881\n",
      "batch 746: loss 0.354810\n",
      "batch 747: loss 0.143475\n",
      "batch 748: loss 0.230899\n",
      "batch 749: loss 0.388890\n",
      "batch 750: loss 0.220920\n",
      "batch 751: loss 0.106947\n",
      "batch 752: loss 0.166309\n",
      "batch 753: loss 0.292352\n",
      "batch 754: loss 0.155596\n",
      "batch 755: loss 0.126391\n",
      "batch 756: loss 0.280672\n",
      "batch 757: loss 0.262782\n",
      "batch 758: loss 0.171407\n",
      "batch 759: loss 0.229026\n",
      "batch 760: loss 0.221524\n",
      "batch 761: loss 0.153927\n",
      "batch 762: loss 0.311587\n",
      "batch 763: loss 0.080126\n",
      "batch 764: loss 0.130316\n",
      "batch 765: loss 0.264166\n",
      "batch 766: loss 0.202977\n",
      "batch 767: loss 0.270893\n",
      "batch 768: loss 0.085312\n",
      "batch 769: loss 0.256486\n",
      "batch 770: loss 0.259014\n",
      "batch 771: loss 0.365903\n",
      "batch 772: loss 0.111429\n",
      "batch 773: loss 0.156459\n",
      "batch 774: loss 0.222035\n",
      "batch 775: loss 0.262915\n",
      "batch 776: loss 0.173159\n",
      "batch 777: loss 0.200505\n",
      "batch 778: loss 0.165460\n",
      "batch 779: loss 0.093181\n",
      "batch 780: loss 0.137883\n",
      "batch 781: loss 0.153624\n",
      "batch 782: loss 0.175544\n",
      "batch 783: loss 0.309363\n",
      "batch 784: loss 0.197875\n",
      "batch 785: loss 0.182591\n",
      "batch 786: loss 0.293419\n",
      "batch 787: loss 0.321114\n",
      "batch 788: loss 0.293997\n",
      "batch 789: loss 0.108050\n",
      "batch 790: loss 0.109509\n",
      "batch 791: loss 0.337097\n",
      "batch 792: loss 0.272777\n",
      "batch 793: loss 0.371229\n",
      "batch 794: loss 0.195284\n",
      "batch 795: loss 0.088356\n",
      "batch 796: loss 0.219673\n",
      "batch 797: loss 0.133493\n",
      "batch 798: loss 0.109172\n",
      "batch 799: loss 0.093172\n",
      "batch 800: loss 0.076883\n",
      "batch 801: loss 0.298526\n",
      "batch 802: loss 0.147638\n",
      "batch 803: loss 0.160311\n",
      "batch 804: loss 0.148105\n",
      "batch 805: loss 0.279993\n",
      "batch 806: loss 0.152899\n",
      "batch 807: loss 0.195735\n",
      "batch 808: loss 0.359921\n",
      "batch 809: loss 0.183837\n",
      "batch 810: loss 0.431430\n",
      "batch 811: loss 0.196556\n",
      "batch 812: loss 0.190470\n",
      "batch 813: loss 0.207860\n",
      "batch 814: loss 0.250560\n",
      "batch 815: loss 0.092431\n",
      "batch 816: loss 0.093350\n",
      "batch 817: loss 0.131451\n",
      "batch 818: loss 0.253402\n",
      "batch 819: loss 0.131000\n",
      "batch 820: loss 0.301119\n",
      "batch 821: loss 0.223709\n",
      "batch 822: loss 0.102766\n",
      "batch 823: loss 0.152128\n",
      "batch 824: loss 0.162361\n",
      "batch 825: loss 0.246929\n",
      "batch 826: loss 0.207605\n",
      "batch 827: loss 0.131830\n",
      "batch 828: loss 0.216642\n",
      "batch 829: loss 0.143039\n",
      "batch 830: loss 0.205670\n",
      "batch 831: loss 0.126864\n",
      "batch 832: loss 0.320369\n",
      "batch 833: loss 0.266997\n",
      "batch 834: loss 0.197123\n",
      "batch 835: loss 0.133270\n",
      "batch 836: loss 0.141994\n",
      "batch 837: loss 0.131565\n",
      "batch 838: loss 0.182684\n",
      "batch 839: loss 0.253157\n",
      "batch 840: loss 0.417804\n",
      "batch 841: loss 0.384811\n",
      "batch 842: loss 0.137216\n",
      "batch 843: loss 0.200921\n",
      "batch 844: loss 0.103194\n",
      "batch 845: loss 0.077772\n",
      "batch 846: loss 0.144932\n",
      "batch 847: loss 0.118532\n",
      "batch 848: loss 0.322861\n",
      "batch 849: loss 0.146590\n",
      "batch 850: loss 0.175694\n",
      "batch 851: loss 0.231959\n",
      "batch 852: loss 0.359558\n",
      "batch 853: loss 0.163987\n",
      "batch 854: loss 0.114811\n",
      "batch 855: loss 0.181512\n",
      "batch 856: loss 0.074854\n",
      "batch 857: loss 0.142638\n",
      "batch 858: loss 0.112954\n",
      "batch 859: loss 0.152008\n",
      "batch 860: loss 0.333227\n",
      "batch 861: loss 0.119979\n",
      "batch 862: loss 0.332500\n",
      "batch 863: loss 0.272354\n",
      "batch 864: loss 0.155683\n",
      "batch 865: loss 0.123374\n",
      "batch 866: loss 0.082962\n",
      "batch 867: loss 0.073695\n",
      "batch 868: loss 0.178722\n",
      "batch 869: loss 0.193965\n",
      "batch 870: loss 0.267572\n",
      "batch 871: loss 0.081041\n",
      "batch 872: loss 0.345354\n",
      "batch 873: loss 0.212005\n",
      "batch 874: loss 0.108139\n",
      "batch 875: loss 0.204516\n",
      "batch 876: loss 0.197091\n",
      "batch 877: loss 0.239946\n",
      "batch 878: loss 0.190937\n",
      "batch 879: loss 0.056825\n",
      "batch 880: loss 0.394548\n",
      "batch 881: loss 0.272344\n",
      "batch 882: loss 0.135671\n",
      "batch 883: loss 0.104619\n",
      "batch 884: loss 0.221076\n",
      "batch 885: loss 0.153383\n",
      "batch 886: loss 0.097143\n",
      "batch 887: loss 0.354755\n",
      "batch 888: loss 0.102435\n",
      "batch 889: loss 0.239018\n",
      "batch 890: loss 0.163160\n",
      "batch 891: loss 0.122803\n",
      "batch 892: loss 0.171931\n",
      "batch 893: loss 0.046728\n",
      "batch 894: loss 0.272366\n",
      "batch 895: loss 0.104019\n",
      "batch 896: loss 0.217294\n",
      "batch 897: loss 0.386957\n",
      "batch 898: loss 0.130386\n",
      "batch 899: loss 0.147909\n",
      "batch 900: loss 0.084297\n",
      "batch 901: loss 0.460380\n",
      "batch 902: loss 0.307538\n",
      "batch 903: loss 0.156632\n",
      "batch 904: loss 0.177565\n",
      "batch 905: loss 0.134103\n",
      "batch 906: loss 0.110539\n",
      "batch 907: loss 0.133966\n",
      "batch 908: loss 0.121008\n",
      "batch 909: loss 0.171696\n",
      "batch 910: loss 0.121608\n",
      "batch 911: loss 0.104326\n",
      "batch 912: loss 0.081910\n",
      "batch 913: loss 0.114028\n",
      "batch 914: loss 0.262395\n",
      "batch 915: loss 0.120492\n",
      "batch 916: loss 0.226181\n",
      "batch 917: loss 0.155714\n",
      "batch 918: loss 0.189635\n",
      "batch 919: loss 0.210700\n",
      "batch 920: loss 0.334910\n",
      "batch 921: loss 0.052255\n",
      "batch 922: loss 0.097930\n",
      "batch 923: loss 0.222911\n",
      "batch 924: loss 0.082699\n",
      "batch 925: loss 0.117349\n",
      "batch 926: loss 0.274493\n",
      "batch 927: loss 0.287916\n",
      "batch 928: loss 0.272555\n",
      "batch 929: loss 0.237161\n",
      "batch 930: loss 0.370490\n",
      "batch 931: loss 0.155829\n",
      "batch 932: loss 0.088613\n",
      "batch 933: loss 0.245374\n",
      "batch 934: loss 0.217638\n",
      "batch 935: loss 0.130614\n",
      "batch 936: loss 0.282492\n",
      "batch 937: loss 0.173624\n",
      "batch 938: loss 0.126605\n",
      "batch 939: loss 0.150930\n",
      "batch 940: loss 0.188951\n",
      "batch 941: loss 0.097088\n",
      "batch 942: loss 0.135456\n",
      "batch 943: loss 0.152586\n",
      "batch 944: loss 0.059515\n",
      "batch 945: loss 0.039928\n",
      "batch 946: loss 0.299183\n",
      "batch 947: loss 0.304476\n",
      "batch 948: loss 0.116263\n",
      "batch 949: loss 0.102699\n",
      "batch 950: loss 0.100310\n",
      "batch 951: loss 0.212315\n",
      "batch 952: loss 0.274338\n",
      "batch 953: loss 0.150609\n",
      "batch 954: loss 0.217386\n",
      "batch 955: loss 0.188168\n",
      "batch 956: loss 0.092456\n",
      "batch 957: loss 0.055099\n",
      "batch 958: loss 0.338266\n",
      "batch 959: loss 0.242326\n",
      "batch 960: loss 0.118006\n",
      "batch 961: loss 0.131395\n",
      "batch 962: loss 0.364984\n",
      "batch 963: loss 0.168128\n",
      "batch 964: loss 0.174338\n",
      "batch 965: loss 0.206793\n",
      "batch 966: loss 0.284800\n",
      "batch 967: loss 0.151043\n",
      "batch 968: loss 0.554795\n",
      "batch 969: loss 0.220135\n",
      "batch 970: loss 0.243262\n",
      "batch 971: loss 0.349439\n",
      "batch 972: loss 0.184411\n",
      "batch 973: loss 0.251574\n",
      "batch 974: loss 0.127541\n",
      "batch 975: loss 0.303782\n",
      "batch 976: loss 0.297303\n",
      "batch 977: loss 0.116885\n",
      "batch 978: loss 0.101163\n",
      "batch 979: loss 0.110340\n",
      "batch 980: loss 0.348297\n",
      "batch 981: loss 0.061053\n",
      "batch 982: loss 0.234374\n",
      "batch 983: loss 0.148669\n",
      "batch 984: loss 0.116218\n",
      "batch 985: loss 0.350206\n",
      "batch 986: loss 0.200204\n",
      "batch 987: loss 0.194919\n",
      "batch 988: loss 0.244793\n",
      "batch 989: loss 0.165841\n",
      "batch 990: loss 0.141444\n",
      "batch 991: loss 0.218599\n",
      "batch 992: loss 0.230797\n",
      "batch 993: loss 0.117244\n",
      "batch 994: loss 0.095424\n",
      "batch 995: loss 0.123379\n",
      "batch 996: loss 0.130441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 997: loss 0.098836\n",
      "batch 998: loss 0.050508\n",
      "batch 999: loss 0.139164\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    for batch_index in range(num_batches):\n",
    "        X, y = data_loader.get_batch(batch_size=batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_logit_pred = model(tf.convert_to_tensor(X))\n",
    "            loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "            \n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.946700\n"
     ]
    }
   ],
   "source": [
    "num_eval_samples = np.shape(data_loader.eval_labels)[0]\n",
    "y_pred = model.predict(data_loader.eval_data).numpy()\n",
    "print(\"test accuracy: %f\" % (sum(y_pred == data_loader.eval_labels) / num_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 卷积神经网络（CNN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,         # 卷积核数目\n",
    "            kernel_size=[5, 5], # 感受野大小\n",
    "            padding=\"same\",     # padding 策略\n",
    "            activation=tf.nn.relu # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "        x = self.conv1(inputs) # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x) # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x) # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x) # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x) # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x) # [batch_size, 1024]\n",
    "        x = self.dense2(x) # [batch_size, 10]\n",
    "        return x\n",
    "    def predict(self, inputs):\n",
    "        logits = self(inputs)\n",
    "        return tf.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将前节的model = MLP() 更换成model2 = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "model2 = CNN()\n",
    "data_loader = DataLoader()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.313076\n",
      "batch 1: loss 2.334512\n",
      "batch 2: loss 2.140350\n",
      "batch 3: loss 1.994281\n",
      "batch 4: loss 1.932082\n",
      "batch 5: loss 1.744478\n",
      "batch 6: loss 1.555119\n",
      "batch 7: loss 1.383521\n",
      "batch 8: loss 1.206875\n",
      "batch 9: loss 1.071436\n",
      "batch 10: loss 0.740621\n",
      "batch 11: loss 0.697872\n",
      "batch 12: loss 0.720873\n",
      "batch 13: loss 0.531402\n",
      "batch 14: loss 0.437014\n",
      "batch 15: loss 0.820353\n",
      "batch 16: loss 0.806522\n",
      "batch 17: loss 0.455345\n",
      "batch 18: loss 0.334382\n",
      "batch 19: loss 0.967779\n",
      "batch 20: loss 0.410174\n",
      "batch 21: loss 0.521947\n",
      "batch 22: loss 0.493575\n",
      "batch 23: loss 1.125018\n",
      "batch 24: loss 0.700175\n",
      "batch 25: loss 0.540875\n",
      "batch 26: loss 0.691857\n",
      "batch 27: loss 0.613570\n",
      "batch 28: loss 0.371598\n",
      "batch 29: loss 0.578266\n",
      "batch 30: loss 0.416864\n",
      "batch 31: loss 0.573842\n",
      "batch 32: loss 0.298479\n",
      "batch 33: loss 0.415747\n",
      "batch 34: loss 0.362678\n",
      "batch 35: loss 0.361590\n",
      "batch 36: loss 0.226848\n",
      "batch 37: loss 0.325481\n",
      "batch 38: loss 0.230098\n",
      "batch 39: loss 0.265313\n",
      "batch 40: loss 0.370976\n",
      "batch 41: loss 0.295844\n",
      "batch 42: loss 0.244073\n",
      "batch 43: loss 0.485298\n",
      "batch 44: loss 0.230261\n",
      "batch 45: loss 0.258681\n",
      "batch 46: loss 0.143018\n",
      "batch 47: loss 0.114708\n",
      "batch 48: loss 0.320661\n",
      "batch 49: loss 0.243158\n",
      "batch 50: loss 0.198892\n",
      "batch 51: loss 0.142275\n",
      "batch 52: loss 0.377120\n",
      "batch 53: loss 0.388732\n",
      "batch 54: loss 0.475230\n",
      "batch 55: loss 0.602778\n",
      "batch 56: loss 0.186289\n",
      "batch 57: loss 0.204439\n",
      "batch 58: loss 0.330948\n",
      "batch 59: loss 0.137181\n",
      "batch 60: loss 0.259403\n",
      "batch 61: loss 0.203380\n",
      "batch 62: loss 0.316907\n",
      "batch 63: loss 0.151134\n",
      "batch 64: loss 0.233867\n",
      "batch 65: loss 0.472557\n",
      "batch 66: loss 0.136506\n",
      "batch 67: loss 0.288933\n",
      "batch 68: loss 0.229534\n",
      "batch 69: loss 0.293531\n",
      "batch 70: loss 0.161682\n",
      "batch 71: loss 0.291837\n",
      "batch 72: loss 0.247710\n",
      "batch 73: loss 0.177315\n",
      "batch 74: loss 0.190053\n",
      "batch 75: loss 0.251112\n",
      "batch 76: loss 0.090726\n",
      "batch 77: loss 0.094785\n",
      "batch 78: loss 0.238638\n",
      "batch 79: loss 0.189934\n",
      "batch 80: loss 0.164918\n",
      "batch 81: loss 0.168553\n",
      "batch 82: loss 0.210865\n",
      "batch 83: loss 0.084055\n",
      "batch 84: loss 0.085802\n",
      "batch 85: loss 0.340077\n",
      "batch 86: loss 0.133373\n",
      "batch 87: loss 0.269679\n",
      "batch 88: loss 0.215606\n",
      "batch 89: loss 0.338321\n",
      "batch 90: loss 0.197970\n",
      "batch 91: loss 0.080930\n",
      "batch 92: loss 0.273236\n",
      "batch 93: loss 0.123148\n",
      "batch 94: loss 0.310955\n",
      "batch 95: loss 0.158331\n",
      "batch 96: loss 0.162077\n",
      "batch 97: loss 0.247556\n",
      "batch 98: loss 0.140453\n",
      "batch 99: loss 0.235356\n",
      "batch 100: loss 0.213872\n",
      "batch 101: loss 0.056522\n",
      "batch 102: loss 0.146533\n",
      "batch 103: loss 0.166608\n",
      "batch 104: loss 0.111498\n",
      "batch 105: loss 0.100099\n",
      "batch 106: loss 0.217468\n",
      "batch 107: loss 0.159723\n",
      "batch 108: loss 0.082162\n",
      "batch 109: loss 0.211431\n",
      "batch 110: loss 0.155040\n",
      "batch 111: loss 0.081847\n",
      "batch 112: loss 0.313023\n",
      "batch 113: loss 0.233574\n",
      "batch 114: loss 0.144103\n",
      "batch 115: loss 0.102594\n",
      "batch 116: loss 0.122579\n",
      "batch 117: loss 0.123011\n",
      "batch 118: loss 0.047291\n",
      "batch 119: loss 0.192627\n",
      "batch 120: loss 0.236040\n",
      "batch 121: loss 0.075601\n",
      "batch 122: loss 0.080679\n",
      "batch 123: loss 0.247347\n",
      "batch 124: loss 0.189413\n",
      "batch 125: loss 0.213759\n",
      "batch 126: loss 0.241147\n",
      "batch 127: loss 0.187455\n",
      "batch 128: loss 0.051250\n",
      "batch 129: loss 0.050019\n",
      "batch 130: loss 0.110133\n",
      "batch 131: loss 0.030688\n",
      "batch 132: loss 0.279968\n",
      "batch 133: loss 0.053628\n",
      "batch 134: loss 0.033548\n",
      "batch 135: loss 0.209154\n",
      "batch 136: loss 0.214935\n",
      "batch 137: loss 0.139116\n",
      "batch 138: loss 0.077003\n",
      "batch 139: loss 0.154154\n",
      "batch 140: loss 0.111830\n",
      "batch 141: loss 0.082362\n",
      "batch 142: loss 0.132976\n",
      "batch 143: loss 0.136916\n",
      "batch 144: loss 0.056409\n",
      "batch 145: loss 0.148335\n",
      "batch 146: loss 0.186155\n",
      "batch 147: loss 0.189607\n",
      "batch 148: loss 0.075045\n",
      "batch 149: loss 0.024072\n",
      "batch 150: loss 0.280087\n",
      "batch 151: loss 0.225139\n",
      "batch 152: loss 0.038183\n",
      "batch 153: loss 0.156894\n",
      "batch 154: loss 0.039833\n",
      "batch 155: loss 0.026818\n",
      "batch 156: loss 0.036220\n",
      "batch 157: loss 0.171142\n",
      "batch 158: loss 0.035323\n",
      "batch 159: loss 0.069583\n",
      "batch 160: loss 0.200888\n",
      "batch 161: loss 0.149285\n",
      "batch 162: loss 0.178032\n",
      "batch 163: loss 0.141534\n",
      "batch 164: loss 0.062841\n",
      "batch 165: loss 0.061337\n",
      "batch 166: loss 0.212565\n",
      "batch 167: loss 0.017195\n",
      "batch 168: loss 0.137596\n",
      "batch 169: loss 0.086779\n",
      "batch 170: loss 0.036386\n",
      "batch 171: loss 0.074326\n",
      "batch 172: loss 0.053496\n",
      "batch 173: loss 0.021128\n",
      "batch 174: loss 0.022558\n",
      "batch 175: loss 0.061357\n",
      "batch 176: loss 0.097881\n",
      "batch 177: loss 0.058034\n",
      "batch 178: loss 0.265809\n",
      "batch 179: loss 0.037145\n",
      "batch 180: loss 0.034910\n",
      "batch 181: loss 0.120329\n",
      "batch 182: loss 0.037640\n",
      "batch 183: loss 0.080575\n",
      "batch 184: loss 0.102652\n",
      "batch 185: loss 0.190939\n",
      "batch 186: loss 0.146345\n",
      "batch 187: loss 0.018864\n",
      "batch 188: loss 0.099205\n",
      "batch 189: loss 0.031215\n",
      "batch 190: loss 0.024951\n",
      "batch 191: loss 0.227917\n",
      "batch 192: loss 0.066988\n",
      "batch 193: loss 0.112116\n",
      "batch 194: loss 0.073238\n",
      "batch 195: loss 0.112785\n",
      "batch 196: loss 0.110046\n",
      "batch 197: loss 0.103877\n",
      "batch 198: loss 0.060679\n",
      "batch 199: loss 0.074418\n",
      "batch 200: loss 0.070845\n",
      "batch 201: loss 0.045477\n",
      "batch 202: loss 0.017842\n",
      "batch 203: loss 0.031079\n",
      "batch 204: loss 0.071189\n",
      "batch 205: loss 0.072651\n",
      "batch 206: loss 0.049656\n",
      "batch 207: loss 0.208993\n",
      "batch 208: loss 0.019209\n",
      "batch 209: loss 0.149261\n",
      "batch 210: loss 0.079605\n",
      "batch 211: loss 0.095157\n",
      "batch 212: loss 0.190521\n",
      "batch 213: loss 0.028828\n",
      "batch 214: loss 0.037517\n",
      "batch 215: loss 0.067353\n",
      "batch 216: loss 0.088425\n",
      "batch 217: loss 0.040847\n",
      "batch 218: loss 0.020754\n",
      "batch 219: loss 0.264801\n",
      "batch 220: loss 0.077746\n",
      "batch 221: loss 0.180771\n",
      "batch 222: loss 0.084647\n",
      "batch 223: loss 0.152469\n",
      "batch 224: loss 0.091144\n",
      "batch 225: loss 0.123902\n",
      "batch 226: loss 0.063895\n",
      "batch 227: loss 0.076866\n",
      "batch 228: loss 0.244804\n",
      "batch 229: loss 0.091889\n",
      "batch 230: loss 0.040223\n",
      "batch 231: loss 0.100410\n",
      "batch 232: loss 0.181073\n",
      "batch 233: loss 0.164453\n",
      "batch 234: loss 0.135851\n",
      "batch 235: loss 0.232291\n",
      "batch 236: loss 0.078702\n",
      "batch 237: loss 0.103483\n",
      "batch 238: loss 0.057485\n",
      "batch 239: loss 0.102234\n",
      "batch 240: loss 0.146454\n",
      "batch 241: loss 0.170637\n",
      "batch 242: loss 0.030880\n",
      "batch 243: loss 0.135175\n",
      "batch 244: loss 0.203103\n",
      "batch 245: loss 0.203190\n",
      "batch 246: loss 0.040467\n",
      "batch 247: loss 0.041483\n",
      "batch 248: loss 0.026139\n",
      "batch 249: loss 0.123245\n",
      "batch 250: loss 0.039298\n",
      "batch 251: loss 0.181001\n",
      "batch 252: loss 0.045920\n",
      "batch 253: loss 0.137190\n",
      "batch 254: loss 0.191618\n",
      "batch 255: loss 0.016983\n",
      "batch 256: loss 0.035012\n",
      "batch 257: loss 0.134745\n",
      "batch 258: loss 0.043834\n",
      "batch 259: loss 0.081909\n",
      "batch 260: loss 0.013643\n",
      "batch 261: loss 0.059842\n",
      "batch 262: loss 0.236975\n",
      "batch 263: loss 0.054100\n",
      "batch 264: loss 0.134151\n",
      "batch 265: loss 0.094950\n",
      "batch 266: loss 0.095975\n",
      "batch 267: loss 0.056272\n",
      "batch 268: loss 0.145470\n",
      "batch 269: loss 0.018166\n",
      "batch 270: loss 0.057511\n",
      "batch 271: loss 0.021695\n",
      "batch 272: loss 0.149299\n",
      "batch 273: loss 0.076057\n",
      "batch 274: loss 0.100640\n",
      "batch 275: loss 0.077727\n",
      "batch 276: loss 0.265599\n",
      "batch 277: loss 0.031273\n",
      "batch 278: loss 0.227691\n",
      "batch 279: loss 0.152446\n",
      "batch 280: loss 0.197236\n",
      "batch 281: loss 0.114220\n",
      "batch 282: loss 0.212572\n",
      "batch 283: loss 0.068488\n",
      "batch 284: loss 0.099234\n",
      "batch 285: loss 0.158515\n",
      "batch 286: loss 0.055346\n",
      "batch 287: loss 0.040719\n",
      "batch 288: loss 0.026392\n",
      "batch 289: loss 0.028042\n",
      "batch 290: loss 0.033277\n",
      "batch 291: loss 0.080082\n",
      "batch 292: loss 0.071754\n",
      "batch 293: loss 0.073035\n",
      "batch 294: loss 0.221461\n",
      "batch 295: loss 0.149124\n",
      "batch 296: loss 0.038816\n",
      "batch 297: loss 0.144999\n",
      "batch 298: loss 0.017388\n",
      "batch 299: loss 0.056250\n",
      "batch 300: loss 0.180582\n",
      "batch 301: loss 0.008275\n",
      "batch 302: loss 0.066572\n",
      "batch 303: loss 0.044141\n",
      "batch 304: loss 0.063405\n",
      "batch 305: loss 0.126237\n",
      "batch 306: loss 0.082033\n",
      "batch 307: loss 0.046432\n",
      "batch 308: loss 0.117807\n",
      "batch 309: loss 0.104770\n",
      "batch 310: loss 0.111534\n",
      "batch 311: loss 0.123818\n",
      "batch 312: loss 0.026119\n",
      "batch 313: loss 0.013496\n",
      "batch 314: loss 0.185697\n",
      "batch 315: loss 0.032782\n",
      "batch 316: loss 0.043957\n",
      "batch 317: loss 0.055256\n",
      "batch 318: loss 0.232405\n",
      "batch 319: loss 0.052781\n",
      "batch 320: loss 0.243447\n",
      "batch 321: loss 0.133184\n",
      "batch 322: loss 0.051008\n",
      "batch 323: loss 0.111336\n",
      "batch 324: loss 0.153631\n",
      "batch 325: loss 0.061736\n",
      "batch 326: loss 0.214812\n",
      "batch 327: loss 0.034638\n",
      "batch 328: loss 0.023680\n",
      "batch 329: loss 0.064800\n",
      "batch 330: loss 0.035153\n",
      "batch 331: loss 0.057464\n",
      "batch 332: loss 0.191981\n",
      "batch 333: loss 0.097266\n",
      "batch 334: loss 0.049063\n",
      "batch 335: loss 0.150146\n",
      "batch 336: loss 0.020745\n",
      "batch 337: loss 0.188083\n",
      "batch 338: loss 0.044117\n",
      "batch 339: loss 0.079943\n",
      "batch 340: loss 0.023535\n",
      "batch 341: loss 0.014576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 342: loss 0.056781\n",
      "batch 343: loss 0.190600\n",
      "batch 344: loss 0.050576\n",
      "batch 345: loss 0.062887\n",
      "batch 346: loss 0.037103\n",
      "batch 347: loss 0.118086\n",
      "batch 348: loss 0.227358\n",
      "batch 349: loss 0.014895\n",
      "batch 350: loss 0.082501\n",
      "batch 351: loss 0.033323\n",
      "batch 352: loss 0.142010\n",
      "batch 353: loss 0.125484\n",
      "batch 354: loss 0.024872\n",
      "batch 355: loss 0.052466\n",
      "batch 356: loss 0.025583\n",
      "batch 357: loss 0.084452\n",
      "batch 358: loss 0.026613\n",
      "batch 359: loss 0.019467\n",
      "batch 360: loss 0.042542\n",
      "batch 361: loss 0.159955\n",
      "batch 362: loss 0.232016\n",
      "batch 363: loss 0.062585\n",
      "batch 364: loss 0.156663\n",
      "batch 365: loss 0.103675\n",
      "batch 366: loss 0.072838\n",
      "batch 367: loss 0.046700\n",
      "batch 368: loss 0.205735\n",
      "batch 369: loss 0.192297\n",
      "batch 370: loss 0.020429\n",
      "batch 371: loss 0.021127\n",
      "batch 372: loss 0.049197\n",
      "batch 373: loss 0.030965\n",
      "batch 374: loss 0.102678\n",
      "batch 375: loss 0.056391\n",
      "batch 376: loss 0.038551\n",
      "batch 377: loss 0.102481\n",
      "batch 378: loss 0.037679\n",
      "batch 379: loss 0.015923\n",
      "batch 380: loss 0.029778\n",
      "batch 381: loss 0.085484\n",
      "batch 382: loss 0.063323\n",
      "batch 383: loss 0.085447\n",
      "batch 384: loss 0.030952\n",
      "batch 385: loss 0.054820\n",
      "batch 386: loss 0.116940\n",
      "batch 387: loss 0.147618\n",
      "batch 388: loss 0.014558\n",
      "batch 389: loss 0.075797\n",
      "batch 390: loss 0.010419\n",
      "batch 391: loss 0.272093\n",
      "batch 392: loss 0.050773\n",
      "batch 393: loss 0.097721\n",
      "batch 394: loss 0.235385\n",
      "batch 395: loss 0.096574\n",
      "batch 396: loss 0.002994\n",
      "batch 397: loss 0.141612\n",
      "batch 398: loss 0.122123\n",
      "batch 399: loss 0.229044\n",
      "batch 400: loss 0.064314\n",
      "batch 401: loss 0.036315\n",
      "batch 402: loss 0.038179\n",
      "batch 403: loss 0.215540\n",
      "batch 404: loss 0.073617\n",
      "batch 405: loss 0.058121\n",
      "batch 406: loss 0.070715\n",
      "batch 407: loss 0.111687\n",
      "batch 408: loss 0.055735\n",
      "batch 409: loss 0.055632\n",
      "batch 410: loss 0.020682\n",
      "batch 411: loss 0.058298\n",
      "batch 412: loss 0.055226\n",
      "batch 413: loss 0.026289\n",
      "batch 414: loss 0.072740\n",
      "batch 415: loss 0.063436\n",
      "batch 416: loss 0.054437\n",
      "batch 417: loss 0.067163\n",
      "batch 418: loss 0.057809\n",
      "batch 419: loss 0.184144\n",
      "batch 420: loss 0.158280\n",
      "batch 421: loss 0.041266\n",
      "batch 422: loss 0.069757\n",
      "batch 423: loss 0.060748\n",
      "batch 424: loss 0.014504\n",
      "batch 425: loss 0.017078\n",
      "batch 426: loss 0.044060\n",
      "batch 427: loss 0.231189\n",
      "batch 428: loss 0.144125\n",
      "batch 429: loss 0.037761\n",
      "batch 430: loss 0.017462\n",
      "batch 431: loss 0.064384\n",
      "batch 432: loss 0.055145\n",
      "batch 433: loss 0.139532\n",
      "batch 434: loss 0.051819\n",
      "batch 435: loss 0.007477\n",
      "batch 436: loss 0.097242\n",
      "batch 437: loss 0.038967\n",
      "batch 438: loss 0.119632\n",
      "batch 439: loss 0.113278\n",
      "batch 440: loss 0.042906\n",
      "batch 441: loss 0.088085\n",
      "batch 442: loss 0.298707\n",
      "batch 443: loss 0.028435\n",
      "batch 444: loss 0.003306\n",
      "batch 445: loss 0.044584\n",
      "batch 446: loss 0.140525\n",
      "batch 447: loss 0.094696\n",
      "batch 448: loss 0.143332\n",
      "batch 449: loss 0.140017\n",
      "batch 450: loss 0.062787\n",
      "batch 451: loss 0.089255\n",
      "batch 452: loss 0.036578\n",
      "batch 453: loss 0.193202\n",
      "batch 454: loss 0.159777\n",
      "batch 455: loss 0.199802\n",
      "batch 456: loss 0.054274\n",
      "batch 457: loss 0.093925\n",
      "batch 458: loss 0.008350\n",
      "batch 459: loss 0.018470\n",
      "batch 460: loss 0.040407\n",
      "batch 461: loss 0.030800\n",
      "batch 462: loss 0.037545\n",
      "batch 463: loss 0.081384\n",
      "batch 464: loss 0.190875\n",
      "batch 465: loss 0.118733\n",
      "batch 466: loss 0.068145\n",
      "batch 467: loss 0.042541\n",
      "batch 468: loss 0.038711\n",
      "batch 469: loss 0.132668\n",
      "batch 470: loss 0.191045\n",
      "batch 471: loss 0.091610\n",
      "batch 472: loss 0.098142\n",
      "batch 473: loss 0.113462\n",
      "batch 474: loss 0.051413\n",
      "batch 475: loss 0.054444\n",
      "batch 476: loss 0.013126\n",
      "batch 477: loss 0.050088\n",
      "batch 478: loss 0.053780\n",
      "batch 479: loss 0.118200\n",
      "batch 480: loss 0.035451\n",
      "batch 481: loss 0.013528\n",
      "batch 482: loss 0.069747\n",
      "batch 483: loss 0.005714\n",
      "batch 484: loss 0.023915\n",
      "batch 485: loss 0.009112\n",
      "batch 486: loss 0.009841\n",
      "batch 487: loss 0.015973\n",
      "batch 488: loss 0.034665\n",
      "batch 489: loss 0.047332\n",
      "batch 490: loss 0.056814\n",
      "batch 491: loss 0.122749\n",
      "batch 492: loss 0.115798\n",
      "batch 493: loss 0.125758\n",
      "batch 494: loss 0.003950\n",
      "batch 495: loss 0.020986\n",
      "batch 496: loss 0.007355\n",
      "batch 497: loss 0.003566\n",
      "batch 498: loss 0.046721\n",
      "batch 499: loss 0.076018\n",
      "batch 500: loss 0.111362\n",
      "batch 501: loss 0.044634\n",
      "batch 502: loss 0.009805\n",
      "batch 503: loss 0.012994\n",
      "batch 504: loss 0.019250\n",
      "batch 505: loss 0.104208\n",
      "batch 506: loss 0.010868\n",
      "batch 507: loss 0.004716\n",
      "batch 508: loss 0.092631\n",
      "batch 509: loss 0.108960\n",
      "batch 510: loss 0.043665\n",
      "batch 511: loss 0.023500\n",
      "batch 512: loss 0.008232\n",
      "batch 513: loss 0.192630\n",
      "batch 514: loss 0.115254\n",
      "batch 515: loss 0.014446\n",
      "batch 516: loss 0.242488\n",
      "batch 517: loss 0.034657\n",
      "batch 518: loss 0.006718\n",
      "batch 519: loss 0.013869\n",
      "batch 520: loss 0.151171\n",
      "batch 521: loss 0.061078\n",
      "batch 522: loss 0.070447\n",
      "batch 523: loss 0.033637\n",
      "batch 524: loss 0.004010\n",
      "batch 525: loss 0.006979\n",
      "batch 526: loss 0.028190\n",
      "batch 527: loss 0.028911\n",
      "batch 528: loss 0.077897\n",
      "batch 529: loss 0.001961\n",
      "batch 530: loss 0.156266\n",
      "batch 531: loss 0.119709\n",
      "batch 532: loss 0.110190\n",
      "batch 533: loss 0.060961\n",
      "batch 534: loss 0.095262\n",
      "batch 535: loss 0.019940\n",
      "batch 536: loss 0.147222\n",
      "batch 537: loss 0.024046\n",
      "batch 538: loss 0.079882\n",
      "batch 539: loss 0.015329\n",
      "batch 540: loss 0.013439\n",
      "batch 541: loss 0.029810\n",
      "batch 542: loss 0.065821\n",
      "batch 543: loss 0.058493\n",
      "batch 544: loss 0.104806\n",
      "batch 545: loss 0.029433\n",
      "batch 546: loss 0.005770\n",
      "batch 547: loss 0.134386\n",
      "batch 548: loss 0.068169\n",
      "batch 549: loss 0.205856\n",
      "batch 550: loss 0.147529\n",
      "batch 551: loss 0.046646\n",
      "batch 552: loss 0.064612\n",
      "batch 553: loss 0.082632\n",
      "batch 554: loss 0.065541\n",
      "batch 555: loss 0.084162\n",
      "batch 556: loss 0.010604\n",
      "batch 557: loss 0.029393\n",
      "batch 558: loss 0.116242\n",
      "batch 559: loss 0.010928\n",
      "batch 560: loss 0.053133\n",
      "batch 561: loss 0.030593\n",
      "batch 562: loss 0.126997\n",
      "batch 563: loss 0.047844\n",
      "batch 564: loss 0.055980\n",
      "batch 565: loss 0.057736\n",
      "batch 566: loss 0.061157\n",
      "batch 567: loss 0.150392\n",
      "batch 568: loss 0.050467\n",
      "batch 569: loss 0.048500\n",
      "batch 570: loss 0.037554\n",
      "batch 571: loss 0.034396\n",
      "batch 572: loss 0.019633\n",
      "batch 573: loss 0.067494\n",
      "batch 574: loss 0.002045\n",
      "batch 575: loss 0.028592\n",
      "batch 576: loss 0.073309\n",
      "batch 577: loss 0.020759\n",
      "batch 578: loss 0.061717\n",
      "batch 579: loss 0.035938\n",
      "batch 580: loss 0.016595\n",
      "batch 581: loss 0.010458\n",
      "batch 582: loss 0.468968\n",
      "batch 583: loss 0.113877\n",
      "batch 584: loss 0.005014\n",
      "batch 585: loss 0.026970\n",
      "batch 586: loss 0.048358\n",
      "batch 587: loss 0.015795\n",
      "batch 588: loss 0.015151\n",
      "batch 589: loss 0.006926\n",
      "batch 590: loss 0.105026\n",
      "batch 591: loss 0.017825\n",
      "batch 592: loss 0.055942\n",
      "batch 593: loss 0.069852\n",
      "batch 594: loss 0.086278\n",
      "batch 595: loss 0.072557\n",
      "batch 596: loss 0.006653\n",
      "batch 597: loss 0.044021\n",
      "batch 598: loss 0.044225\n",
      "batch 599: loss 0.006813\n",
      "batch 600: loss 0.134506\n",
      "batch 601: loss 0.061992\n",
      "batch 602: loss 0.280261\n",
      "batch 603: loss 0.043388\n",
      "batch 604: loss 0.024630\n",
      "batch 605: loss 0.048812\n",
      "batch 606: loss 0.264432\n",
      "batch 607: loss 0.006309\n",
      "batch 608: loss 0.026030\n",
      "batch 609: loss 0.081257\n",
      "batch 610: loss 0.034946\n",
      "batch 611: loss 0.122730\n",
      "batch 612: loss 0.094352\n",
      "batch 613: loss 0.034741\n",
      "batch 614: loss 0.016997\n",
      "batch 615: loss 0.031296\n",
      "batch 616: loss 0.012567\n",
      "batch 617: loss 0.021132\n",
      "batch 618: loss 0.021261\n",
      "batch 619: loss 0.090689\n",
      "batch 620: loss 0.040003\n",
      "batch 621: loss 0.070239\n",
      "batch 622: loss 0.023935\n",
      "batch 623: loss 0.057722\n",
      "batch 624: loss 0.014167\n",
      "batch 625: loss 0.119012\n",
      "batch 626: loss 0.009063\n",
      "batch 627: loss 0.011189\n",
      "batch 628: loss 0.041039\n",
      "batch 629: loss 0.057126\n",
      "batch 630: loss 0.040096\n",
      "batch 631: loss 0.138666\n",
      "batch 632: loss 0.015788\n",
      "batch 633: loss 0.041617\n",
      "batch 634: loss 0.003913\n",
      "batch 635: loss 0.067499\n",
      "batch 636: loss 0.358382\n",
      "batch 637: loss 0.040241\n",
      "batch 638: loss 0.008946\n",
      "batch 639: loss 0.042886\n",
      "batch 640: loss 0.053908\n",
      "batch 641: loss 0.010625\n",
      "batch 642: loss 0.183085\n",
      "batch 643: loss 0.057763\n",
      "batch 644: loss 0.035349\n",
      "batch 645: loss 0.017252\n",
      "batch 646: loss 0.092872\n",
      "batch 647: loss 0.024624\n",
      "batch 648: loss 0.116360\n",
      "batch 649: loss 0.078682\n",
      "batch 650: loss 0.026386\n",
      "batch 651: loss 0.017043\n",
      "batch 652: loss 0.017689\n",
      "batch 653: loss 0.046795\n",
      "batch 654: loss 0.103610\n",
      "batch 655: loss 0.027689\n",
      "batch 656: loss 0.039037\n",
      "batch 657: loss 0.048460\n",
      "batch 658: loss 0.004919\n",
      "batch 659: loss 0.035046\n",
      "batch 660: loss 0.019514\n",
      "batch 661: loss 0.065505\n",
      "batch 662: loss 0.062372\n",
      "batch 663: loss 0.066939\n",
      "batch 664: loss 0.018908\n",
      "batch 665: loss 0.113827\n",
      "batch 666: loss 0.005842\n",
      "batch 667: loss 0.024400\n",
      "batch 668: loss 0.007167\n",
      "batch 669: loss 0.017695\n",
      "batch 670: loss 0.028325\n",
      "batch 671: loss 0.072178\n",
      "batch 672: loss 0.020807\n",
      "batch 673: loss 0.157654\n",
      "batch 674: loss 0.046530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 675: loss 0.054163\n",
      "batch 676: loss 0.014256\n",
      "batch 677: loss 0.007424\n",
      "batch 678: loss 0.043015\n",
      "batch 679: loss 0.030288\n",
      "batch 680: loss 0.018242\n",
      "batch 681: loss 0.032942\n",
      "batch 682: loss 0.077418\n",
      "batch 683: loss 0.040841\n",
      "batch 684: loss 0.015264\n",
      "batch 685: loss 0.032322\n",
      "batch 686: loss 0.037506\n",
      "batch 687: loss 0.036361\n",
      "batch 688: loss 0.117720\n",
      "batch 689: loss 0.088070\n",
      "batch 690: loss 0.036908\n",
      "batch 691: loss 0.022117\n",
      "batch 692: loss 0.008762\n",
      "batch 693: loss 0.003912\n",
      "batch 694: loss 0.043463\n",
      "batch 695: loss 0.084042\n",
      "batch 696: loss 0.003475\n",
      "batch 697: loss 0.094564\n",
      "batch 698: loss 0.013938\n",
      "batch 699: loss 0.015577\n",
      "batch 700: loss 0.001040\n",
      "batch 701: loss 0.039178\n",
      "batch 702: loss 0.012809\n",
      "batch 703: loss 0.005088\n",
      "batch 704: loss 0.215752\n",
      "batch 705: loss 0.047267\n",
      "batch 706: loss 0.009188\n",
      "batch 707: loss 0.006410\n",
      "batch 708: loss 0.047753\n",
      "batch 709: loss 0.008027\n",
      "batch 710: loss 0.014539\n",
      "batch 711: loss 0.005208\n",
      "batch 712: loss 0.064951\n",
      "batch 713: loss 0.043398\n",
      "batch 714: loss 0.015884\n",
      "batch 715: loss 0.030679\n",
      "batch 716: loss 0.004111\n",
      "batch 717: loss 0.065425\n",
      "batch 718: loss 0.047635\n",
      "batch 719: loss 0.019775\n",
      "batch 720: loss 0.130494\n",
      "batch 721: loss 0.026490\n",
      "batch 722: loss 0.053024\n",
      "batch 723: loss 0.008939\n",
      "batch 724: loss 0.008630\n",
      "batch 725: loss 0.027000\n",
      "batch 726: loss 0.047084\n",
      "batch 727: loss 0.016768\n",
      "batch 728: loss 0.018516\n",
      "batch 729: loss 0.059432\n",
      "batch 730: loss 0.102266\n",
      "batch 731: loss 0.064243\n",
      "batch 732: loss 0.015169\n",
      "batch 733: loss 0.004492\n",
      "batch 734: loss 0.010115\n",
      "batch 735: loss 0.082490\n",
      "batch 736: loss 0.026569\n",
      "batch 737: loss 0.119861\n",
      "batch 738: loss 0.044338\n",
      "batch 739: loss 0.007994\n",
      "batch 740: loss 0.059262\n",
      "batch 741: loss 0.070076\n",
      "batch 742: loss 0.009826\n",
      "batch 743: loss 0.118162\n",
      "batch 744: loss 0.009501\n",
      "batch 745: loss 0.045246\n",
      "batch 746: loss 0.108201\n",
      "batch 747: loss 0.014685\n",
      "batch 748: loss 0.058134\n",
      "batch 749: loss 0.006353\n",
      "batch 750: loss 0.069737\n",
      "batch 751: loss 0.046448\n",
      "batch 752: loss 0.012012\n",
      "batch 753: loss 0.022584\n",
      "batch 754: loss 0.020374\n",
      "batch 755: loss 0.021647\n",
      "batch 756: loss 0.047041\n",
      "batch 757: loss 0.045041\n",
      "batch 758: loss 0.097105\n",
      "batch 759: loss 0.072890\n",
      "batch 760: loss 0.031344\n",
      "batch 761: loss 0.016273\n",
      "batch 762: loss 0.078277\n",
      "batch 763: loss 0.091429\n",
      "batch 764: loss 0.043606\n",
      "batch 765: loss 0.052860\n",
      "batch 766: loss 0.037626\n",
      "batch 767: loss 0.046903\n",
      "batch 768: loss 0.014008\n",
      "batch 769: loss 0.003342\n",
      "batch 770: loss 0.053780\n",
      "batch 771: loss 0.035786\n",
      "batch 772: loss 0.009114\n",
      "batch 773: loss 0.006596\n",
      "batch 774: loss 0.045098\n",
      "batch 775: loss 0.111723\n",
      "batch 776: loss 0.007671\n",
      "batch 777: loss 0.178004\n",
      "batch 778: loss 0.017921\n",
      "batch 779: loss 0.074059\n",
      "batch 780: loss 0.017187\n",
      "batch 781: loss 0.066250\n",
      "batch 782: loss 0.035738\n",
      "batch 783: loss 0.093886\n",
      "batch 784: loss 0.117895\n",
      "batch 785: loss 0.028531\n",
      "batch 786: loss 0.030492\n",
      "batch 787: loss 0.032428\n",
      "batch 788: loss 0.040594\n",
      "batch 789: loss 0.012329\n",
      "batch 790: loss 0.011559\n",
      "batch 791: loss 0.080031\n",
      "batch 792: loss 0.020433\n",
      "batch 793: loss 0.037731\n",
      "batch 794: loss 0.033264\n",
      "batch 795: loss 0.009172\n",
      "batch 796: loss 0.013063\n",
      "batch 797: loss 0.058434\n",
      "batch 798: loss 0.072208\n",
      "batch 799: loss 0.002593\n",
      "batch 800: loss 0.089230\n",
      "batch 801: loss 0.003110\n",
      "batch 802: loss 0.061170\n",
      "batch 803: loss 0.023036\n",
      "batch 804: loss 0.051856\n",
      "batch 805: loss 0.081822\n",
      "batch 806: loss 0.015065\n",
      "batch 807: loss 0.010350\n",
      "batch 808: loss 0.001649\n",
      "batch 809: loss 0.032909\n",
      "batch 810: loss 0.012352\n",
      "batch 811: loss 0.040796\n",
      "batch 812: loss 0.022599\n",
      "batch 813: loss 0.033218\n",
      "batch 814: loss 0.032787\n",
      "batch 815: loss 0.063792\n",
      "batch 816: loss 0.011207\n",
      "batch 817: loss 0.002146\n",
      "batch 818: loss 0.006870\n",
      "batch 819: loss 0.006612\n",
      "batch 820: loss 0.044893\n",
      "batch 821: loss 0.020837\n",
      "batch 822: loss 0.039242\n",
      "batch 823: loss 0.041183\n",
      "batch 824: loss 0.056640\n",
      "batch 825: loss 0.066637\n",
      "batch 826: loss 0.086183\n",
      "batch 827: loss 0.046242\n",
      "batch 828: loss 0.143396\n",
      "batch 829: loss 0.002471\n",
      "batch 830: loss 0.007947\n",
      "batch 831: loss 0.035085\n",
      "batch 832: loss 0.047195\n",
      "batch 833: loss 0.054430\n",
      "batch 834: loss 0.131728\n",
      "batch 835: loss 0.051620\n",
      "batch 836: loss 0.001591\n",
      "batch 837: loss 0.071072\n",
      "batch 838: loss 0.201133\n",
      "batch 839: loss 0.010800\n",
      "batch 840: loss 0.062321\n",
      "batch 841: loss 0.008805\n",
      "batch 842: loss 0.014331\n",
      "batch 843: loss 0.052074\n",
      "batch 844: loss 0.043781\n",
      "batch 845: loss 0.012946\n",
      "batch 846: loss 0.010694\n",
      "batch 847: loss 0.039979\n",
      "batch 848: loss 0.063273\n",
      "batch 849: loss 0.005945\n",
      "batch 850: loss 0.040661\n",
      "batch 851: loss 0.037720\n",
      "batch 852: loss 0.000884\n",
      "batch 853: loss 0.106165\n",
      "batch 854: loss 0.005486\n",
      "batch 855: loss 0.007613\n",
      "batch 856: loss 0.048301\n",
      "batch 857: loss 0.094565\n",
      "batch 858: loss 0.001000\n",
      "batch 859: loss 0.008965\n",
      "batch 860: loss 0.013344\n",
      "batch 861: loss 0.040090\n",
      "batch 862: loss 0.122484\n",
      "batch 863: loss 0.002086\n",
      "batch 864: loss 0.087775\n",
      "batch 865: loss 0.024878\n",
      "batch 866: loss 0.004923\n",
      "batch 867: loss 0.005552\n",
      "batch 868: loss 0.017373\n",
      "batch 869: loss 0.034260\n",
      "batch 870: loss 0.031768\n",
      "batch 871: loss 0.005768\n",
      "batch 872: loss 0.150149\n",
      "batch 873: loss 0.011663\n",
      "batch 874: loss 0.027744\n",
      "batch 875: loss 0.044606\n",
      "batch 876: loss 0.005829\n",
      "batch 877: loss 0.002125\n",
      "batch 878: loss 0.002514\n",
      "batch 879: loss 0.001298\n",
      "batch 880: loss 0.077467\n",
      "batch 881: loss 0.001581\n",
      "batch 882: loss 0.011151\n",
      "batch 883: loss 0.034474\n",
      "batch 884: loss 0.022286\n",
      "batch 885: loss 0.036675\n",
      "batch 886: loss 0.043185\n",
      "batch 887: loss 0.007326\n",
      "batch 888: loss 0.033891\n",
      "batch 889: loss 0.164395\n",
      "batch 890: loss 0.013951\n",
      "batch 891: loss 0.164724\n",
      "batch 892: loss 0.097354\n",
      "batch 893: loss 0.000735\n",
      "batch 894: loss 0.041886\n",
      "batch 895: loss 0.145131\n",
      "batch 896: loss 0.101700\n",
      "batch 897: loss 0.047316\n",
      "batch 898: loss 0.123428\n",
      "batch 899: loss 0.066651\n",
      "batch 900: loss 0.006812\n",
      "batch 901: loss 0.024302\n",
      "batch 902: loss 0.121400\n",
      "batch 903: loss 0.008221\n",
      "batch 904: loss 0.004131\n",
      "batch 905: loss 0.009010\n",
      "batch 906: loss 0.028863\n",
      "batch 907: loss 0.136240\n",
      "batch 908: loss 0.053894\n",
      "batch 909: loss 0.004587\n",
      "batch 910: loss 0.096464\n",
      "batch 911: loss 0.021259\n",
      "batch 912: loss 0.044616\n",
      "batch 913: loss 0.003512\n",
      "batch 914: loss 0.075977\n",
      "batch 915: loss 0.113051\n",
      "batch 916: loss 0.018724\n",
      "batch 917: loss 0.097324\n",
      "batch 918: loss 0.052605\n",
      "batch 919: loss 0.178815\n",
      "batch 920: loss 0.010685\n",
      "batch 921: loss 0.052050\n",
      "batch 922: loss 0.125198\n",
      "batch 923: loss 0.011264\n",
      "batch 924: loss 0.004181\n",
      "batch 925: loss 0.018310\n",
      "batch 926: loss 0.020464\n",
      "batch 927: loss 0.023140\n",
      "batch 928: loss 0.143209\n",
      "batch 929: loss 0.015996\n",
      "batch 930: loss 0.030197\n",
      "batch 931: loss 0.213955\n",
      "batch 932: loss 0.014794\n",
      "batch 933: loss 0.007779\n",
      "batch 934: loss 0.093438\n",
      "batch 935: loss 0.045655\n",
      "batch 936: loss 0.080220\n",
      "batch 937: loss 0.005967\n",
      "batch 938: loss 0.006893\n",
      "batch 939: loss 0.040593\n",
      "batch 940: loss 0.151231\n",
      "batch 941: loss 0.010349\n",
      "batch 942: loss 0.009810\n",
      "batch 943: loss 0.073446\n",
      "batch 944: loss 0.089755\n",
      "batch 945: loss 0.034189\n",
      "batch 946: loss 0.102160\n",
      "batch 947: loss 0.005164\n",
      "batch 948: loss 0.033174\n",
      "batch 949: loss 0.104319\n",
      "batch 950: loss 0.090448\n",
      "batch 951: loss 0.006184\n",
      "batch 952: loss 0.024212\n",
      "batch 953: loss 0.030412\n",
      "batch 954: loss 0.012857\n",
      "batch 955: loss 0.022821\n",
      "batch 956: loss 0.010690\n",
      "batch 957: loss 0.058326\n",
      "batch 958: loss 0.014146\n",
      "batch 959: loss 0.002388\n",
      "batch 960: loss 0.113776\n",
      "batch 961: loss 0.007831\n",
      "batch 962: loss 0.002757\n",
      "batch 963: loss 0.009434\n",
      "batch 964: loss 0.003116\n",
      "batch 965: loss 0.004914\n",
      "batch 966: loss 0.033442\n",
      "batch 967: loss 0.042225\n",
      "batch 968: loss 0.008216\n",
      "batch 969: loss 0.013206\n",
      "batch 970: loss 0.016661\n",
      "batch 971: loss 0.030844\n",
      "batch 972: loss 0.061324\n",
      "batch 973: loss 0.010017\n",
      "batch 974: loss 0.002863\n",
      "batch 975: loss 0.010719\n",
      "batch 976: loss 0.062678\n",
      "batch 977: loss 0.004627\n",
      "batch 978: loss 0.097975\n",
      "batch 979: loss 0.026385\n",
      "batch 980: loss 0.006650\n",
      "batch 981: loss 0.034337\n",
      "batch 982: loss 0.001921\n",
      "batch 983: loss 0.087813\n",
      "batch 984: loss 0.006615\n",
      "batch 985: loss 0.001235\n",
      "batch 986: loss 0.130251\n",
      "batch 987: loss 0.056657\n",
      "batch 988: loss 0.014746\n",
      "batch 989: loss 0.006406\n",
      "batch 990: loss 0.153772\n",
      "batch 991: loss 0.025878\n",
      "batch 992: loss 0.051010\n",
      "batch 993: loss 0.138632\n",
      "batch 994: loss 0.059037\n",
      "batch 995: loss 0.095577\n",
      "batch 996: loss 0.013522\n",
      "batch 997: loss 0.046972\n",
      "batch 998: loss 0.050378\n",
      "batch 999: loss 0.040401\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    for batch_index in range(num_batches):\n",
    "        X, y = data_loader.get_batch(batch_size=batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_logit_pred = model2(tf.convert_to_tensor(X))\n",
    "            loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "            \n",
    "        grads = tape.gradient(loss, model2.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model2.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.987700\n"
     ]
    }
   ],
   "source": [
    "num_eval_samples = np.shape(data_loader.eval_labels)[0]\n",
    "y_pred = model2.predict(data_loader.eval_data).numpy()\n",
    "print(\"test accuracy: %f\" % (sum(y_pred == data_loader.eval_labels) / num_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 循环神经网络（RNN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a example of simple encoding the **Text --> Numerical_Value (char-wise)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Text: [2, 0, 3, 12, 0, 9, 3, 15, 15, 19, 0, 3, 4, 14, 18, 17, 0, 4, 7, 10, 13, 8, 0, 3, 0, 6, 14, 8, 0, 14, 16, 0, 3, 0, 5, 3, 17, 0, 14, 16, 0, 17, 18, 16, 17, 11, 7, 1] \n",
      "\n",
      "Plain Text: I am happy about being a dog or a cat or turtle.\n"
     ]
    }
   ],
   "source": [
    "# character-wise (letter-wise) splitting \n",
    "text = 'I am happy about being a dog or a cat or turtle.'\n",
    "chars = sorted(list(set(text)))\n",
    "#print(chars)\n",
    "\n",
    "# to encode each char as Index\n",
    "char_index = dict((c, i) for i, c in enumerate(chars))\n",
    "#print(char_index)\n",
    "\n",
    "# the reversed encoding\n",
    "index_char = dict((i, c) for i, c in enumerate(chars))\n",
    "#print(index_char)\n",
    "\n",
    "# to encode the raw_text as numerical number\n",
    "idx_text = [char_index[c] for c in text]\n",
    "print('Encoded Text:', idx_text, '\\n')\n",
    "print('Plain Text:', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataLoader():\n",
    "    \n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "    \n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index:index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "        return np.array(seq), np.array(next_char)  # [num_batch, seq_length], [num_batch]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_chars):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=512)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size, seq_length = tf.shape(inputs)\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)  # by One-hot, inputs becomes [batch_size, seq_length, num_chars]\n",
    "        \n",
    "        state = self.cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        for t in range(seq_length.numpy()):\n",
    "            output, state = self.cell(inputs[:, t, :], state)\n",
    "        output = self.dense(output)\n",
    "        return output      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Same training process as previous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "seq_length = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-52d85fd94d15>:6: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextDataLoader()\n",
    "model = RNN(len(data_loader.chars))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 4.041428\n",
      "batch 1: loss 4.013526\n",
      "batch 2: loss 3.990120\n",
      "batch 3: loss 3.944863\n",
      "batch 4: loss 3.867048\n",
      "batch 5: loss 3.719494\n",
      "batch 6: loss 3.457839\n",
      "batch 7: loss 3.808088\n",
      "batch 8: loss 2.964936\n",
      "batch 9: loss 3.361090\n",
      "batch 10: loss 3.242783\n",
      "batch 11: loss 3.217525\n",
      "batch 12: loss 3.395174\n",
      "batch 13: loss 3.109822\n",
      "batch 14: loss 3.250504\n",
      "batch 15: loss 3.199307\n",
      "batch 16: loss 3.109925\n",
      "batch 17: loss 3.362569\n",
      "batch 18: loss 3.020768\n",
      "batch 19: loss 3.391301\n",
      "batch 20: loss 3.449104\n",
      "batch 21: loss 3.018502\n",
      "batch 22: loss 3.058741\n",
      "batch 23: loss 3.294282\n",
      "batch 24: loss 3.213609\n",
      "batch 25: loss 3.071831\n",
      "batch 26: loss 2.962432\n",
      "batch 27: loss 3.125489\n",
      "batch 28: loss 2.948421\n",
      "batch 29: loss 3.386027\n",
      "batch 30: loss 3.349418\n",
      "batch 31: loss 3.310807\n",
      "batch 32: loss 2.974597\n",
      "batch 33: loss 3.281292\n",
      "batch 34: loss 2.879974\n",
      "batch 35: loss 3.355217\n",
      "batch 36: loss 3.185140\n",
      "batch 37: loss 2.983382\n",
      "batch 38: loss 3.113577\n",
      "batch 39: loss 3.101875\n",
      "batch 40: loss 2.886891\n",
      "batch 41: loss 2.987552\n",
      "batch 42: loss 2.954515\n",
      "batch 43: loss 3.242393\n",
      "batch 44: loss 2.985411\n",
      "batch 45: loss 3.181935\n",
      "batch 46: loss 2.977611\n",
      "batch 47: loss 2.964597\n",
      "batch 48: loss 3.251667\n",
      "batch 49: loss 3.213562\n",
      "batch 50: loss 3.247368\n",
      "batch 51: loss 2.982095\n",
      "batch 52: loss 3.050056\n",
      "batch 53: loss 3.044927\n",
      "batch 54: loss 3.122508\n",
      "batch 55: loss 2.966642\n",
      "batch 56: loss 3.199656\n",
      "batch 57: loss 3.059863\n",
      "batch 58: loss 3.037113\n",
      "batch 59: loss 2.981745\n",
      "batch 60: loss 3.042374\n",
      "batch 61: loss 2.984403\n",
      "batch 62: loss 3.135028\n",
      "batch 63: loss 3.159145\n",
      "batch 64: loss 3.084038\n",
      "batch 65: loss 2.990467\n",
      "batch 66: loss 2.942179\n",
      "batch 67: loss 2.974963\n",
      "batch 68: loss 3.175517\n",
      "batch 69: loss 3.233908\n",
      "batch 70: loss 2.877245\n",
      "batch 71: loss 3.085809\n",
      "batch 72: loss 2.862333\n",
      "batch 73: loss 3.114241\n",
      "batch 74: loss 3.190312\n",
      "batch 75: loss 3.037784\n",
      "batch 76: loss 2.962290\n",
      "batch 77: loss 2.905697\n",
      "batch 78: loss 3.064539\n",
      "batch 79: loss 3.004094\n",
      "batch 80: loss 3.039981\n",
      "batch 81: loss 2.833657\n",
      "batch 82: loss 3.216888\n",
      "batch 83: loss 2.962792\n",
      "batch 84: loss 3.339838\n",
      "batch 85: loss 3.233898\n",
      "batch 86: loss 3.066491\n",
      "batch 87: loss 3.123362\n",
      "batch 88: loss 3.099106\n",
      "batch 89: loss 2.949107\n",
      "batch 90: loss 3.070970\n",
      "batch 91: loss 2.871949\n",
      "batch 92: loss 2.889400\n",
      "batch 93: loss 3.106920\n",
      "batch 94: loss 3.227052\n",
      "batch 95: loss 3.132134\n",
      "batch 96: loss 3.213880\n",
      "batch 97: loss 3.190752\n",
      "batch 98: loss 2.869601\n",
      "batch 99: loss 3.404968\n",
      "batch 100: loss 2.947231\n",
      "batch 101: loss 2.964992\n",
      "batch 102: loss 2.885355\n",
      "batch 103: loss 3.055625\n",
      "batch 104: loss 3.026218\n",
      "batch 105: loss 2.829167\n",
      "batch 106: loss 3.060422\n",
      "batch 107: loss 2.878360\n",
      "batch 108: loss 3.140758\n",
      "batch 109: loss 3.026086\n",
      "batch 110: loss 3.316752\n",
      "batch 111: loss 2.913368\n",
      "batch 112: loss 2.685185\n",
      "batch 113: loss 3.061680\n",
      "batch 114: loss 2.923578\n",
      "batch 115: loss 3.100177\n",
      "batch 116: loss 2.715770\n",
      "batch 117: loss 2.754219\n",
      "batch 118: loss 2.864431\n",
      "batch 119: loss 3.203460\n",
      "batch 120: loss 2.854694\n",
      "batch 121: loss 2.991580\n",
      "batch 122: loss 2.850077\n",
      "batch 123: loss 2.813885\n",
      "batch 124: loss 2.970218\n",
      "batch 125: loss 2.753751\n",
      "batch 126: loss 3.072150\n",
      "batch 127: loss 3.107359\n",
      "batch 128: loss 3.040897\n",
      "batch 129: loss 2.857160\n",
      "batch 130: loss 2.909704\n",
      "batch 131: loss 3.078456\n",
      "batch 132: loss 3.323246\n",
      "batch 133: loss 2.916761\n",
      "batch 134: loss 2.954080\n",
      "batch 135: loss 3.032387\n",
      "batch 136: loss 2.759026\n",
      "batch 137: loss 2.979723\n",
      "batch 138: loss 3.178064\n",
      "batch 139: loss 2.950611\n",
      "batch 140: loss 2.743590\n",
      "batch 141: loss 3.061911\n",
      "batch 142: loss 3.003371\n",
      "batch 143: loss 2.867482\n",
      "batch 144: loss 2.763478\n",
      "batch 145: loss 3.009311\n",
      "batch 146: loss 2.966008\n",
      "batch 147: loss 3.010049\n",
      "batch 148: loss 2.834761\n",
      "batch 149: loss 3.188184\n",
      "batch 150: loss 2.826144\n",
      "batch 151: loss 2.829539\n",
      "batch 152: loss 3.037776\n",
      "batch 153: loss 2.802675\n",
      "batch 154: loss 2.829718\n",
      "batch 155: loss 2.976227\n",
      "batch 156: loss 2.951583\n",
      "batch 157: loss 2.931310\n",
      "batch 158: loss 2.894229\n",
      "batch 159: loss 3.034124\n",
      "batch 160: loss 2.913242\n",
      "batch 161: loss 2.987402\n",
      "batch 162: loss 3.238147\n",
      "batch 163: loss 2.853312\n",
      "batch 164: loss 3.134241\n",
      "batch 165: loss 3.091113\n",
      "batch 166: loss 2.825168\n",
      "batch 167: loss 3.295874\n",
      "batch 168: loss 3.020778\n",
      "batch 169: loss 2.941712\n",
      "batch 170: loss 2.950269\n",
      "batch 171: loss 2.899283\n",
      "batch 172: loss 3.010644\n",
      "batch 173: loss 2.969860\n",
      "batch 174: loss 2.938725\n",
      "batch 175: loss 3.035984\n",
      "batch 176: loss 2.984902\n",
      "batch 177: loss 2.831161\n",
      "batch 178: loss 2.706219\n",
      "batch 179: loss 3.062989\n",
      "batch 180: loss 2.997252\n",
      "batch 181: loss 2.900429\n",
      "batch 182: loss 2.985612\n",
      "batch 183: loss 2.846022\n",
      "batch 184: loss 2.753320\n",
      "batch 185: loss 2.924907\n",
      "batch 186: loss 2.947889\n",
      "batch 187: loss 2.976302\n",
      "batch 188: loss 3.129334\n",
      "batch 189: loss 2.812209\n",
      "batch 190: loss 3.116354\n",
      "batch 191: loss 3.092343\n",
      "batch 192: loss 3.066902\n",
      "batch 193: loss 2.946311\n",
      "batch 194: loss 2.745260\n",
      "batch 195: loss 2.898718\n",
      "batch 196: loss 2.961303\n",
      "batch 197: loss 2.943118\n",
      "batch 198: loss 2.658141\n",
      "batch 199: loss 2.941421\n",
      "batch 200: loss 2.879838\n",
      "batch 201: loss 2.936406\n",
      "batch 202: loss 3.266646\n",
      "batch 203: loss 2.724155\n",
      "batch 204: loss 3.089101\n",
      "batch 205: loss 2.703012\n",
      "batch 206: loss 2.881448\n",
      "batch 207: loss 2.828640\n",
      "batch 208: loss 2.826329\n",
      "batch 209: loss 2.864837\n",
      "batch 210: loss 2.851537\n",
      "batch 211: loss 2.911694\n",
      "batch 212: loss 3.081815\n",
      "batch 213: loss 2.677930\n",
      "batch 214: loss 2.887617\n",
      "batch 215: loss 2.747147\n",
      "batch 216: loss 3.111252\n",
      "batch 217: loss 2.836316\n",
      "batch 218: loss 2.606309\n",
      "batch 219: loss 2.950168\n",
      "batch 220: loss 3.075596\n",
      "batch 221: loss 2.805707\n",
      "batch 222: loss 3.282265\n",
      "batch 223: loss 3.118070\n",
      "batch 224: loss 2.856606\n",
      "batch 225: loss 3.138177\n",
      "batch 226: loss 2.897719\n",
      "batch 227: loss 2.951085\n",
      "batch 228: loss 2.936921\n",
      "batch 229: loss 2.652530\n",
      "batch 230: loss 3.053533\n",
      "batch 231: loss 3.040977\n",
      "batch 232: loss 3.082355\n",
      "batch 233: loss 2.743345\n",
      "batch 234: loss 2.926292\n",
      "batch 235: loss 2.642943\n",
      "batch 236: loss 2.810017\n",
      "batch 237: loss 2.805668\n",
      "batch 238: loss 2.804636\n",
      "batch 239: loss 2.628325\n",
      "batch 240: loss 3.045938\n",
      "batch 241: loss 2.915744\n",
      "batch 242: loss 2.519631\n",
      "batch 243: loss 2.646414\n",
      "batch 244: loss 2.856792\n",
      "batch 245: loss 2.748572\n",
      "batch 246: loss 3.238369\n",
      "batch 247: loss 2.669245\n",
      "batch 248: loss 2.845474\n",
      "batch 249: loss 2.639218\n",
      "batch 250: loss 2.740705\n",
      "batch 251: loss 2.895246\n",
      "batch 252: loss 2.706950\n",
      "batch 253: loss 2.766512\n",
      "batch 254: loss 2.860507\n",
      "batch 255: loss 3.095602\n",
      "batch 256: loss 2.743663\n",
      "batch 257: loss 2.702995\n",
      "batch 258: loss 2.946396\n",
      "batch 259: loss 2.774137\n",
      "batch 260: loss 2.780007\n",
      "batch 261: loss 2.834310\n",
      "batch 262: loss 2.863747\n",
      "batch 263: loss 3.083274\n",
      "batch 264: loss 2.874982\n",
      "batch 265: loss 2.914459\n",
      "batch 266: loss 2.992015\n",
      "batch 267: loss 2.698910\n",
      "batch 268: loss 2.837073\n",
      "batch 269: loss 2.864130\n",
      "batch 270: loss 2.425404\n",
      "batch 271: loss 2.734941\n",
      "batch 272: loss 2.808877\n",
      "batch 273: loss 2.934339\n",
      "batch 274: loss 2.680377\n",
      "batch 275: loss 2.653167\n",
      "batch 276: loss 2.597590\n",
      "batch 277: loss 2.446955\n",
      "batch 278: loss 2.645605\n",
      "batch 279: loss 2.972332\n",
      "batch 280: loss 2.463175\n",
      "batch 281: loss 2.869607\n",
      "batch 282: loss 2.770504\n",
      "batch 283: loss 2.804930\n",
      "batch 284: loss 2.725299\n",
      "batch 285: loss 2.758116\n",
      "batch 286: loss 2.585670\n",
      "batch 287: loss 2.605353\n",
      "batch 288: loss 2.692659\n",
      "batch 289: loss 2.867396\n",
      "batch 290: loss 2.598068\n",
      "batch 291: loss 2.569198\n",
      "batch 292: loss 2.752959\n",
      "batch 293: loss 2.651776\n",
      "batch 294: loss 2.977263\n",
      "batch 295: loss 2.744886\n",
      "batch 296: loss 2.654806\n",
      "batch 297: loss 2.710875\n",
      "batch 298: loss 3.024491\n",
      "batch 299: loss 3.083241\n",
      "batch 300: loss 2.523465\n",
      "batch 301: loss 2.683342\n",
      "batch 302: loss 2.556072\n",
      "batch 303: loss 2.600035\n",
      "batch 304: loss 2.840300\n",
      "batch 305: loss 2.688616\n",
      "batch 306: loss 2.662261\n",
      "batch 307: loss 2.687748\n",
      "batch 308: loss 2.529562\n",
      "batch 309: loss 3.036852\n",
      "batch 310: loss 2.739704\n",
      "batch 311: loss 2.658735\n",
      "batch 312: loss 2.779683\n",
      "batch 313: loss 2.893484\n",
      "batch 314: loss 2.713276\n",
      "batch 315: loss 2.786154\n",
      "batch 316: loss 2.728093\n",
      "batch 317: loss 2.944995\n",
      "batch 318: loss 2.710510\n",
      "batch 319: loss 2.630362\n",
      "batch 320: loss 2.576438\n",
      "batch 321: loss 2.574712\n",
      "batch 322: loss 2.893801\n",
      "batch 323: loss 2.456555\n",
      "batch 324: loss 2.775899\n",
      "batch 325: loss 2.896404\n",
      "batch 326: loss 2.495951\n",
      "batch 327: loss 2.701332\n",
      "batch 328: loss 2.617369\n",
      "batch 329: loss 2.832779\n",
      "batch 330: loss 2.554960\n",
      "batch 331: loss 2.677929\n",
      "batch 332: loss 2.622403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 333: loss 2.933625\n",
      "batch 334: loss 2.889222\n",
      "batch 335: loss 2.484551\n",
      "batch 336: loss 2.798637\n",
      "batch 337: loss 2.805980\n",
      "batch 338: loss 2.591292\n",
      "batch 339: loss 2.636713\n",
      "batch 340: loss 2.644470\n",
      "batch 341: loss 2.593138\n",
      "batch 342: loss 2.684174\n",
      "batch 343: loss 2.621077\n",
      "batch 344: loss 2.485885\n",
      "batch 345: loss 3.035900\n",
      "batch 346: loss 3.042144\n",
      "batch 347: loss 2.633136\n",
      "batch 348: loss 2.627552\n",
      "batch 349: loss 2.807768\n",
      "batch 350: loss 2.488351\n",
      "batch 351: loss 2.500343\n",
      "batch 352: loss 2.886507\n",
      "batch 353: loss 2.758011\n",
      "batch 354: loss 2.681462\n",
      "batch 355: loss 2.572367\n",
      "batch 356: loss 2.584578\n",
      "batch 357: loss 2.334399\n",
      "batch 358: loss 2.821466\n",
      "batch 359: loss 2.648486\n",
      "batch 360: loss 2.739994\n",
      "batch 361: loss 2.674418\n",
      "batch 362: loss 2.509083\n",
      "batch 363: loss 2.765704\n",
      "batch 364: loss 2.600050\n",
      "batch 365: loss 2.816675\n",
      "batch 366: loss 2.494140\n",
      "batch 367: loss 2.942632\n",
      "batch 368: loss 2.570733\n",
      "batch 369: loss 2.489734\n",
      "batch 370: loss 2.500167\n",
      "batch 371: loss 2.546912\n",
      "batch 372: loss 2.858124\n",
      "batch 373: loss 2.313947\n",
      "batch 374: loss 2.432546\n",
      "batch 375: loss 2.595480\n",
      "batch 376: loss 2.452070\n",
      "batch 377: loss 2.921434\n",
      "batch 378: loss 2.913897\n",
      "batch 379: loss 2.384856\n",
      "batch 380: loss 2.664312\n",
      "batch 381: loss 2.624250\n",
      "batch 382: loss 2.326835\n",
      "batch 383: loss 2.579220\n",
      "batch 384: loss 2.565049\n",
      "batch 385: loss 2.856105\n",
      "batch 386: loss 2.553428\n",
      "batch 387: loss 2.326107\n",
      "batch 388: loss 2.765108\n",
      "batch 389: loss 2.579232\n",
      "batch 390: loss 2.416193\n",
      "batch 391: loss 2.583581\n",
      "batch 392: loss 2.371780\n",
      "batch 393: loss 2.618136\n",
      "batch 394: loss 2.602027\n",
      "batch 395: loss 2.622094\n",
      "batch 396: loss 2.683079\n",
      "batch 397: loss 2.568036\n",
      "batch 398: loss 2.801839\n",
      "batch 399: loss 2.796339\n",
      "batch 400: loss 2.825707\n",
      "batch 401: loss 2.352477\n",
      "batch 402: loss 2.835476\n",
      "batch 403: loss 2.679357\n",
      "batch 404: loss 2.676053\n",
      "batch 405: loss 2.471636\n",
      "batch 406: loss 2.627702\n",
      "batch 407: loss 2.673370\n",
      "batch 408: loss 2.508268\n",
      "batch 409: loss 2.706113\n",
      "batch 410: loss 2.799479\n",
      "batch 411: loss 2.549363\n",
      "batch 412: loss 2.625950\n",
      "batch 413: loss 2.751841\n",
      "batch 414: loss 2.553926\n",
      "batch 415: loss 2.550906\n",
      "batch 416: loss 2.970044\n",
      "batch 417: loss 2.645193\n",
      "batch 418: loss 2.352585\n",
      "batch 419: loss 2.859685\n",
      "batch 420: loss 2.689143\n",
      "batch 421: loss 2.332610\n",
      "batch 422: loss 2.714410\n",
      "batch 423: loss 2.573795\n",
      "batch 424: loss 2.510202\n",
      "batch 425: loss 2.669113\n",
      "batch 426: loss 2.446522\n",
      "batch 427: loss 2.491914\n",
      "batch 428: loss 2.450546\n",
      "batch 429: loss 2.645223\n",
      "batch 430: loss 2.721401\n",
      "batch 431: loss 2.715538\n",
      "batch 432: loss 2.673185\n",
      "batch 433: loss 2.603627\n",
      "batch 434: loss 2.885215\n",
      "batch 435: loss 2.422761\n",
      "batch 436: loss 2.343570\n",
      "batch 437: loss 2.615136\n",
      "batch 438: loss 2.315620\n",
      "batch 439: loss 2.465879\n",
      "batch 440: loss 2.446757\n",
      "batch 441: loss 2.735073\n",
      "batch 442: loss 2.468367\n",
      "batch 443: loss 2.430615\n",
      "batch 444: loss 2.233703\n",
      "batch 445: loss 2.650359\n",
      "batch 446: loss 2.541045\n",
      "batch 447: loss 2.485180\n",
      "batch 448: loss 2.343226\n",
      "batch 449: loss 2.284094\n",
      "batch 450: loss 2.944996\n",
      "batch 451: loss 2.497268\n",
      "batch 452: loss 2.693308\n",
      "batch 453: loss 2.619801\n",
      "batch 454: loss 2.788073\n",
      "batch 455: loss 2.500430\n",
      "batch 456: loss 2.572249\n",
      "batch 457: loss 2.923350\n",
      "batch 458: loss 2.780699\n",
      "batch 459: loss 2.428422\n",
      "batch 460: loss 2.651902\n",
      "batch 461: loss 2.594132\n",
      "batch 462: loss 2.362190\n",
      "batch 463: loss 2.463393\n",
      "batch 464: loss 2.488767\n",
      "batch 465: loss 2.478555\n",
      "batch 466: loss 2.507394\n",
      "batch 467: loss 2.637800\n",
      "batch 468: loss 2.785783\n",
      "batch 469: loss 2.695628\n",
      "batch 470: loss 2.643701\n",
      "batch 471: loss 2.462930\n",
      "batch 472: loss 2.488828\n",
      "batch 473: loss 2.559741\n",
      "batch 474: loss 2.584738\n",
      "batch 475: loss 2.562551\n",
      "batch 476: loss 2.569452\n",
      "batch 477: loss 2.729174\n",
      "batch 478: loss 2.535247\n",
      "batch 479: loss 2.639195\n",
      "batch 480: loss 2.199439\n",
      "batch 481: loss 2.412231\n",
      "batch 482: loss 2.897497\n",
      "batch 483: loss 2.463886\n",
      "batch 484: loss 2.832273\n",
      "batch 485: loss 2.206689\n",
      "batch 486: loss 2.463951\n",
      "batch 487: loss 2.844682\n",
      "batch 488: loss 2.657146\n",
      "batch 489: loss 2.677096\n",
      "batch 490: loss 2.791027\n",
      "batch 491: loss 2.360981\n",
      "batch 492: loss 2.948967\n",
      "batch 493: loss 2.497136\n",
      "batch 494: loss 2.436062\n",
      "batch 495: loss 2.376078\n",
      "batch 496: loss 2.452311\n",
      "batch 497: loss 2.708569\n",
      "batch 498: loss 2.786311\n",
      "batch 499: loss 2.389056\n",
      "batch 500: loss 2.274551\n",
      "batch 501: loss 2.314733\n",
      "batch 502: loss 2.283815\n",
      "batch 503: loss 2.407091\n",
      "batch 504: loss 2.389530\n",
      "batch 505: loss 2.735441\n",
      "batch 506: loss 2.336551\n",
      "batch 507: loss 2.869338\n",
      "batch 508: loss 2.712771\n",
      "batch 509: loss 2.515491\n",
      "batch 510: loss 2.660687\n",
      "batch 511: loss 2.103406\n",
      "batch 512: loss 2.241070\n",
      "batch 513: loss 2.629925\n",
      "batch 514: loss 2.565778\n",
      "batch 515: loss 2.229452\n",
      "batch 516: loss 2.376458\n",
      "batch 517: loss 2.708941\n",
      "batch 518: loss 2.784815\n",
      "batch 519: loss 2.599458\n",
      "batch 520: loss 2.744726\n",
      "batch 521: loss 2.371565\n",
      "batch 522: loss 2.484334\n",
      "batch 523: loss 2.695362\n",
      "batch 524: loss 2.371903\n",
      "batch 525: loss 2.575090\n",
      "batch 526: loss 2.467038\n",
      "batch 527: loss 2.596452\n",
      "batch 528: loss 2.536482\n",
      "batch 529: loss 2.428195\n",
      "batch 530: loss 2.445138\n",
      "batch 531: loss 2.505969\n",
      "batch 532: loss 2.336102\n",
      "batch 533: loss 2.782708\n",
      "batch 534: loss 2.363842\n",
      "batch 535: loss 2.479455\n",
      "batch 536: loss 2.268100\n",
      "batch 537: loss 2.640160\n",
      "batch 538: loss 2.430466\n",
      "batch 539: loss 2.428932\n",
      "batch 540: loss 2.574129\n",
      "batch 541: loss 2.657693\n",
      "batch 542: loss 2.723866\n",
      "batch 543: loss 2.721786\n",
      "batch 544: loss 2.559343\n",
      "batch 545: loss 2.552235\n",
      "batch 546: loss 2.712193\n",
      "batch 547: loss 2.608366\n",
      "batch 548: loss 2.465396\n",
      "batch 549: loss 2.525031\n",
      "batch 550: loss 2.151028\n",
      "batch 551: loss 2.537428\n",
      "batch 552: loss 2.379473\n",
      "batch 553: loss 2.284233\n",
      "batch 554: loss 2.499431\n",
      "batch 555: loss 2.364280\n",
      "batch 556: loss 2.472611\n",
      "batch 557: loss 2.459378\n",
      "batch 558: loss 2.598937\n",
      "batch 559: loss 2.446863\n",
      "batch 560: loss 2.534705\n",
      "batch 561: loss 2.733838\n",
      "batch 562: loss 2.339518\n",
      "batch 563: loss 2.397388\n",
      "batch 564: loss 2.767030\n",
      "batch 565: loss 2.329498\n",
      "batch 566: loss 2.387160\n",
      "batch 567: loss 2.602736\n",
      "batch 568: loss 2.546184\n",
      "batch 569: loss 2.344989\n",
      "batch 570: loss 2.458666\n",
      "batch 571: loss 2.337025\n",
      "batch 572: loss 2.305599\n",
      "batch 573: loss 2.250679\n",
      "batch 574: loss 2.496309\n",
      "batch 575: loss 2.501864\n",
      "batch 576: loss 2.427941\n",
      "batch 577: loss 2.432460\n",
      "batch 578: loss 2.404081\n",
      "batch 579: loss 2.444113\n",
      "batch 580: loss 2.312659\n",
      "batch 581: loss 2.440841\n",
      "batch 582: loss 2.735985\n",
      "batch 583: loss 2.393891\n",
      "batch 584: loss 2.217356\n",
      "batch 585: loss 2.567719\n",
      "batch 586: loss 2.835275\n",
      "batch 587: loss 2.882278\n",
      "batch 588: loss 2.392778\n",
      "batch 589: loss 2.297932\n",
      "batch 590: loss 2.560859\n",
      "batch 591: loss 2.519681\n",
      "batch 592: loss 2.564900\n",
      "batch 593: loss 2.480172\n",
      "batch 594: loss 2.738067\n",
      "batch 595: loss 2.316729\n",
      "batch 596: loss 2.346164\n",
      "batch 597: loss 2.576829\n",
      "batch 598: loss 2.707069\n",
      "batch 599: loss 2.701729\n",
      "batch 600: loss 2.524669\n",
      "batch 601: loss 2.494212\n",
      "batch 602: loss 2.462203\n",
      "batch 603: loss 2.785387\n",
      "batch 604: loss 2.546544\n",
      "batch 605: loss 2.316736\n",
      "batch 606: loss 2.424625\n",
      "batch 607: loss 2.664851\n",
      "batch 608: loss 2.354033\n",
      "batch 609: loss 2.584432\n",
      "batch 610: loss 2.528969\n",
      "batch 611: loss 2.699538\n",
      "batch 612: loss 2.365147\n",
      "batch 613: loss 2.629780\n",
      "batch 614: loss 2.498592\n",
      "batch 615: loss 2.807852\n",
      "batch 616: loss 2.634625\n",
      "batch 617: loss 2.649120\n",
      "batch 618: loss 2.378250\n",
      "batch 619: loss 2.424475\n",
      "batch 620: loss 2.355538\n",
      "batch 621: loss 2.606304\n",
      "batch 622: loss 2.704826\n",
      "batch 623: loss 2.544293\n",
      "batch 624: loss 2.149364\n",
      "batch 625: loss 2.361990\n",
      "batch 626: loss 2.536239\n",
      "batch 627: loss 2.454533\n",
      "batch 628: loss 2.482037\n",
      "batch 629: loss 2.282095\n",
      "batch 630: loss 2.457461\n",
      "batch 631: loss 2.187190\n",
      "batch 632: loss 2.483485\n",
      "batch 633: loss 2.274390\n",
      "batch 634: loss 2.420961\n",
      "batch 635: loss 2.374094\n",
      "batch 636: loss 2.345880\n",
      "batch 637: loss 2.284099\n",
      "batch 638: loss 2.752127\n",
      "batch 639: loss 2.397416\n",
      "batch 640: loss 2.146530\n",
      "batch 641: loss 2.565988\n",
      "batch 642: loss 2.405964\n",
      "batch 643: loss 2.284030\n",
      "batch 644: loss 2.390692\n",
      "batch 645: loss 2.346661\n",
      "batch 646: loss 2.431729\n",
      "batch 647: loss 2.428268\n",
      "batch 648: loss 2.894652\n",
      "batch 649: loss 2.494574\n",
      "batch 650: loss 2.112478\n",
      "batch 651: loss 2.628425\n",
      "batch 652: loss 2.370059\n",
      "batch 653: loss 2.518451\n",
      "batch 654: loss 2.378914\n",
      "batch 655: loss 2.339793\n",
      "batch 656: loss 2.348905\n",
      "batch 657: loss 2.829197\n",
      "batch 658: loss 2.370955\n",
      "batch 659: loss 2.491727\n",
      "batch 660: loss 2.402838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 661: loss 2.654325\n",
      "batch 662: loss 2.601650\n",
      "batch 663: loss 2.561137\n",
      "batch 664: loss 2.128028\n",
      "batch 665: loss 2.441259\n",
      "batch 666: loss 2.557840\n",
      "batch 667: loss 2.798706\n",
      "batch 668: loss 2.869058\n",
      "batch 669: loss 2.473774\n",
      "batch 670: loss 2.520879\n",
      "batch 671: loss 2.142156\n",
      "batch 672: loss 2.392135\n",
      "batch 673: loss 2.349168\n",
      "batch 674: loss 2.342052\n",
      "batch 675: loss 2.569873\n",
      "batch 676: loss 2.641418\n",
      "batch 677: loss 2.476115\n",
      "batch 678: loss 2.562207\n",
      "batch 679: loss 2.253854\n",
      "batch 680: loss 2.603491\n",
      "batch 681: loss 2.262234\n",
      "batch 682: loss 2.504150\n",
      "batch 683: loss 2.530632\n",
      "batch 684: loss 2.233327\n",
      "batch 685: loss 2.753789\n",
      "batch 686: loss 2.545019\n",
      "batch 687: loss 2.311787\n",
      "batch 688: loss 2.531698\n",
      "batch 689: loss 2.561783\n",
      "batch 690: loss 2.455875\n",
      "batch 691: loss 2.280603\n",
      "batch 692: loss 2.410181\n",
      "batch 693: loss 2.394200\n",
      "batch 694: loss 2.517167\n",
      "batch 695: loss 2.501303\n",
      "batch 696: loss 2.343846\n",
      "batch 697: loss 2.559388\n",
      "batch 698: loss 2.573536\n",
      "batch 699: loss 2.404645\n",
      "batch 700: loss 2.359545\n",
      "batch 701: loss 2.402872\n",
      "batch 702: loss 2.165131\n",
      "batch 703: loss 2.846876\n",
      "batch 704: loss 2.625910\n",
      "batch 705: loss 2.508034\n",
      "batch 706: loss 2.517820\n",
      "batch 707: loss 2.580308\n",
      "batch 708: loss 2.792167\n",
      "batch 709: loss 2.355042\n",
      "batch 710: loss 2.748818\n",
      "batch 711: loss 2.536606\n",
      "batch 712: loss 2.415906\n",
      "batch 713: loss 2.591269\n",
      "batch 714: loss 2.459080\n",
      "batch 715: loss 2.482438\n",
      "batch 716: loss 2.892842\n",
      "batch 717: loss 2.219756\n",
      "batch 718: loss 2.466629\n",
      "batch 719: loss 2.311341\n",
      "batch 720: loss 2.323276\n",
      "batch 721: loss 2.089131\n",
      "batch 722: loss 2.652502\n",
      "batch 723: loss 2.442927\n",
      "batch 724: loss 2.536031\n",
      "batch 725: loss 2.690850\n",
      "batch 726: loss 2.444701\n",
      "batch 727: loss 2.345012\n",
      "batch 728: loss 2.461288\n",
      "batch 729: loss 2.143690\n",
      "batch 730: loss 2.565445\n",
      "batch 731: loss 2.612663\n",
      "batch 732: loss 1.959837\n",
      "batch 733: loss 2.459959\n",
      "batch 734: loss 2.705982\n",
      "batch 735: loss 2.399547\n",
      "batch 736: loss 2.645145\n",
      "batch 737: loss 2.497983\n",
      "batch 738: loss 2.403933\n",
      "batch 739: loss 2.431383\n",
      "batch 740: loss 2.539871\n",
      "batch 741: loss 2.725143\n",
      "batch 742: loss 2.181195\n",
      "batch 743: loss 2.731683\n",
      "batch 744: loss 2.425374\n",
      "batch 745: loss 2.407335\n",
      "batch 746: loss 2.359185\n",
      "batch 747: loss 2.445232\n",
      "batch 748: loss 2.547313\n",
      "batch 749: loss 2.495809\n",
      "batch 750: loss 2.367180\n",
      "batch 751: loss 2.504862\n",
      "batch 752: loss 2.236088\n",
      "batch 753: loss 2.643928\n",
      "batch 754: loss 2.565646\n",
      "batch 755: loss 2.419106\n",
      "batch 756: loss 2.309388\n",
      "batch 757: loss 2.159007\n",
      "batch 758: loss 2.410446\n",
      "batch 759: loss 2.496183\n",
      "batch 760: loss 2.552745\n",
      "batch 761: loss 2.348579\n",
      "batch 762: loss 2.158399\n",
      "batch 763: loss 2.056619\n",
      "batch 764: loss 2.416334\n",
      "batch 765: loss 2.523427\n",
      "batch 766: loss 2.399981\n",
      "batch 767: loss 2.090856\n",
      "batch 768: loss 2.419890\n",
      "batch 769: loss 2.071066\n",
      "batch 770: loss 2.610584\n",
      "batch 771: loss 2.348819\n",
      "batch 772: loss 2.399662\n",
      "batch 773: loss 2.358908\n",
      "batch 774: loss 2.349558\n",
      "batch 775: loss 2.379369\n",
      "batch 776: loss 2.056864\n",
      "batch 777: loss 2.464561\n",
      "batch 778: loss 2.502075\n",
      "batch 779: loss 2.185873\n",
      "batch 780: loss 2.616027\n",
      "batch 781: loss 2.156707\n",
      "batch 782: loss 2.295661\n",
      "batch 783: loss 2.186821\n",
      "batch 784: loss 2.231871\n",
      "batch 785: loss 2.749500\n",
      "batch 786: loss 2.388283\n",
      "batch 787: loss 2.420759\n",
      "batch 788: loss 2.403392\n",
      "batch 789: loss 2.563295\n",
      "batch 790: loss 2.266417\n",
      "batch 791: loss 2.497672\n",
      "batch 792: loss 2.450733\n",
      "batch 793: loss 2.286514\n",
      "batch 794: loss 2.598586\n",
      "batch 795: loss 2.494446\n",
      "batch 796: loss 2.514196\n",
      "batch 797: loss 2.552173\n",
      "batch 798: loss 2.377589\n",
      "batch 799: loss 2.345894\n",
      "batch 800: loss 2.539409\n",
      "batch 801: loss 2.540823\n",
      "batch 802: loss 2.797916\n",
      "batch 803: loss 2.168881\n",
      "batch 804: loss 2.331049\n",
      "batch 805: loss 2.347244\n",
      "batch 806: loss 2.553408\n",
      "batch 807: loss 2.402946\n",
      "batch 808: loss 2.322256\n",
      "batch 809: loss 2.499562\n",
      "batch 810: loss 2.640664\n",
      "batch 811: loss 2.320931\n",
      "batch 812: loss 2.580183\n",
      "batch 813: loss 2.392796\n",
      "batch 814: loss 2.177122\n",
      "batch 815: loss 2.006918\n",
      "batch 816: loss 2.079580\n",
      "batch 817: loss 2.746823\n",
      "batch 818: loss 2.615990\n",
      "batch 819: loss 2.385453\n",
      "batch 820: loss 2.319207\n",
      "batch 821: loss 2.448025\n",
      "batch 822: loss 2.249933\n",
      "batch 823: loss 2.519804\n",
      "batch 824: loss 2.222594\n",
      "batch 825: loss 2.178028\n",
      "batch 826: loss 2.151351\n",
      "batch 827: loss 2.358110\n",
      "batch 828: loss 2.233634\n",
      "batch 829: loss 2.217051\n",
      "batch 830: loss 2.439518\n",
      "batch 831: loss 2.347093\n",
      "batch 832: loss 2.564363\n",
      "batch 833: loss 2.096366\n",
      "batch 834: loss 2.244924\n",
      "batch 835: loss 2.254485\n",
      "batch 836: loss 2.108032\n",
      "batch 837: loss 2.374480\n",
      "batch 838: loss 2.331586\n",
      "batch 839: loss 2.336063\n",
      "batch 840: loss 2.206346\n",
      "batch 841: loss 2.333810\n",
      "batch 842: loss 2.168693\n",
      "batch 843: loss 2.082521\n",
      "batch 844: loss 2.276345\n",
      "batch 845: loss 2.261110\n",
      "batch 846: loss 2.276174\n",
      "batch 847: loss 2.338342\n",
      "batch 848: loss 2.150959\n",
      "batch 849: loss 2.433605\n",
      "batch 850: loss 2.587950\n",
      "batch 851: loss 2.260134\n",
      "batch 852: loss 2.053053\n",
      "batch 853: loss 2.689805\n",
      "batch 854: loss 2.404429\n",
      "batch 855: loss 2.615487\n",
      "batch 856: loss 2.385849\n",
      "batch 857: loss 2.296058\n",
      "batch 858: loss 2.373790\n",
      "batch 859: loss 2.512546\n",
      "batch 860: loss 2.542685\n",
      "batch 861: loss 2.528759\n",
      "batch 862: loss 2.601763\n",
      "batch 863: loss 2.203421\n",
      "batch 864: loss 2.508814\n",
      "batch 865: loss 2.680751\n",
      "batch 866: loss 2.409163\n",
      "batch 867: loss 2.415063\n",
      "batch 868: loss 2.230544\n",
      "batch 869: loss 2.375729\n",
      "batch 870: loss 2.439739\n",
      "batch 871: loss 1.908554\n",
      "batch 872: loss 2.414063\n",
      "batch 873: loss 2.308612\n",
      "batch 874: loss 2.377923\n",
      "batch 875: loss 2.594947\n",
      "batch 876: loss 2.379535\n",
      "batch 877: loss 2.413073\n",
      "batch 878: loss 2.352221\n",
      "batch 879: loss 2.421654\n",
      "batch 880: loss 2.415007\n",
      "batch 881: loss 2.505346\n",
      "batch 882: loss 2.497279\n",
      "batch 883: loss 2.369457\n",
      "batch 884: loss 2.009609\n",
      "batch 885: loss 2.427813\n",
      "batch 886: loss 2.204377\n",
      "batch 887: loss 2.255552\n",
      "batch 888: loss 2.867330\n",
      "batch 889: loss 2.278651\n",
      "batch 890: loss 2.574242\n",
      "batch 891: loss 2.532118\n",
      "batch 892: loss 2.395157\n",
      "batch 893: loss 2.362292\n",
      "batch 894: loss 2.339603\n",
      "batch 895: loss 2.254233\n",
      "batch 896: loss 2.259331\n",
      "batch 897: loss 2.218152\n",
      "batch 898: loss 2.590871\n",
      "batch 899: loss 2.262745\n",
      "batch 900: loss 2.554900\n",
      "batch 901: loss 2.012161\n",
      "batch 902: loss 2.404205\n",
      "batch 903: loss 2.354918\n",
      "batch 904: loss 2.174183\n",
      "batch 905: loss 2.382146\n",
      "batch 906: loss 2.489335\n",
      "batch 907: loss 2.124873\n",
      "batch 908: loss 2.038327\n",
      "batch 909: loss 2.086475\n",
      "batch 910: loss 2.339120\n",
      "batch 911: loss 2.053511\n",
      "batch 912: loss 2.560532\n",
      "batch 913: loss 2.316446\n",
      "batch 914: loss 2.286780\n",
      "batch 915: loss 2.318880\n",
      "batch 916: loss 2.118250\n",
      "batch 917: loss 2.249963\n",
      "batch 918: loss 2.696599\n",
      "batch 919: loss 2.446633\n",
      "batch 920: loss 2.296394\n",
      "batch 921: loss 2.300469\n",
      "batch 922: loss 2.239804\n",
      "batch 923: loss 2.121185\n",
      "batch 924: loss 2.550172\n",
      "batch 925: loss 2.388379\n",
      "batch 926: loss 2.354860\n",
      "batch 927: loss 2.444560\n",
      "batch 928: loss 2.564541\n",
      "batch 929: loss 1.968918\n",
      "batch 930: loss 2.258461\n",
      "batch 931: loss 2.355116\n",
      "batch 932: loss 2.060295\n",
      "batch 933: loss 2.146402\n",
      "batch 934: loss 2.652426\n",
      "batch 935: loss 2.453920\n",
      "batch 936: loss 2.543787\n",
      "batch 937: loss 2.361034\n",
      "batch 938: loss 2.231785\n",
      "batch 939: loss 2.229146\n",
      "batch 940: loss 2.431048\n",
      "batch 941: loss 2.378731\n",
      "batch 942: loss 2.160255\n",
      "batch 943: loss 2.478039\n",
      "batch 944: loss 2.300259\n",
      "batch 945: loss 2.504482\n",
      "batch 946: loss 2.086326\n",
      "batch 947: loss 2.360168\n",
      "batch 948: loss 2.509262\n",
      "batch 949: loss 2.005701\n",
      "batch 950: loss 2.099118\n",
      "batch 951: loss 2.194656\n",
      "batch 952: loss 2.113898\n",
      "batch 953: loss 2.128994\n",
      "batch 954: loss 2.488627\n",
      "batch 955: loss 2.328822\n",
      "batch 956: loss 2.719811\n",
      "batch 957: loss 2.506566\n",
      "batch 958: loss 2.602605\n",
      "batch 959: loss 2.093989\n",
      "batch 960: loss 2.390364\n",
      "batch 961: loss 2.166782\n",
      "batch 962: loss 2.714533\n",
      "batch 963: loss 2.315277\n",
      "batch 964: loss 2.182894\n",
      "batch 965: loss 2.232577\n",
      "batch 966: loss 2.228569\n",
      "batch 967: loss 2.253006\n",
      "batch 968: loss 2.394742\n",
      "batch 969: loss 2.124990\n",
      "batch 970: loss 2.367149\n",
      "batch 971: loss 2.245775\n",
      "batch 972: loss 1.951075\n",
      "batch 973: loss 2.064233\n",
      "batch 974: loss 2.187817\n",
      "batch 975: loss 2.423800\n",
      "batch 976: loss 2.353973\n",
      "batch 977: loss 2.306385\n",
      "batch 978: loss 2.116405\n",
      "batch 979: loss 2.096850\n",
      "batch 980: loss 2.010495\n",
      "batch 981: loss 2.233271\n",
      "batch 982: loss 2.449280\n",
      "batch 983: loss 2.449764\n",
      "batch 984: loss 2.377911\n",
      "batch 985: loss 2.278810\n",
      "batch 986: loss 2.622905\n",
      "batch 987: loss 2.232418\n",
      "batch 988: loss 2.383736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 989: loss 2.294607\n",
      "batch 990: loss 2.300985\n",
      "batch 991: loss 2.831500\n",
      "batch 992: loss 2.454754\n",
      "batch 993: loss 2.370755\n",
      "batch 994: loss 2.116971\n",
      "batch 995: loss 2.402585\n",
      "batch 996: loss 2.525663\n",
      "batch 997: loss 2.415970\n",
      "batch 998: loss 2.348136\n",
      "batch 999: loss 2.313159\n"
     ]
    }
   ],
   "source": [
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_logit_pred = model(X)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, inputs, temperature=1.):\n",
    "    batch_size, _ = tf.shape(inputs)\n",
    "    logits = self(inputs)\n",
    "    prob = tf.nn.softmax(logits / temperature).numpy()\n",
    "    return np.array([np.random.choice(self.num_chars, p=prob[i, :]) for i in range(batch_size.numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity 0.200000:\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Could not find valid device for node.\nNode: {{node OneHot}} = OneHot[T=DT_FLOAT, TI=DT_FLOAT, axis=-1](dummy_input, dummy_input, dummy_input, dummy_input)\nAll kernels registered for op OneHot :\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT32]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT64]; T in [DT_STRING]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_VARIANT]\n [Op:OneHot] name: one_hot/",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bf4f06d34b0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"diversity %f:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     \u001b[1;31m# means that we end up calculating it twice which we should avoid.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1864\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[1;32m--> 992\u001b[1;33m                                                      class_weight, batch_size)\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   1030\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0mis_build_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m       \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_symbolic_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m       \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_eager_set_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[0mdummy_input_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_input_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m     \u001b[0mdummy_output_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_input_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_symbolic_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_single_as_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-52d85fd94d15>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# by One-hot, inputs becomes [batch_size, seq_length, num_chars]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[1;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[0;32m   2443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2444\u001b[0m     return gen_array_ops.one_hot(indices, depth, on_value, off_value, axis,\n\u001b[1;32m-> 2445\u001b[1;33m                                  name)\n\u001b[0m\u001b[0;32m   2446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[1;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[0;32m   5709\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5710\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5711\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Could not find valid device for node.\nNode: {{node OneHot}} = OneHot[T=DT_FLOAT, TI=DT_FLOAT, axis=-1](dummy_input, dummy_input, dummy_input, dummy_input)\nAll kernels registered for op OneHot :\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT32]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT64]; T in [DT_STRING]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_VARIANT]\n [Op:OneHot] name: one_hot/"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "for diversity in [0.2]:\n",
    "    X = X_\n",
    "    print(\"diversity %f:\" % diversity)\n",
    "    for t in range(400):\n",
    "        y_pred = model.predict(X, diversity)\n",
    "        print(data_loader.indices_char[y_pred[0]], end='', flush=True)\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
